<!DOCTYPE html> <html lang="en"> <head> <meta name="google-site-verification" content=""/> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>JAFAR: Jack up Any Feature at Any Resolution | valeo.ai - valeo.ai research page</title> <meta name="author" content=" "/> <meta name="description" content="valeo.ai research page "/> <meta name="keywords" content="computer vision, ai, valeo, artificial intelligence, research, deep learning"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light"/> <link rel="shortcut icon" href="/assets/img/valeoai_logo_256x256.png"/> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://valeoai.github.io//publications/jafar/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous"> <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"> </script> <script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"></script> <script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"[%",right:"%]",display:!0},{left:"$",right:"$",display:!1}]})});</script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><img src="/assets/img/valeoai_logo.png" alt="valeo.ai" class="title-logo" height="24px"></a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/"></a> </li> <li class="nav-item "> <a class="nav-link" href="/team/">Team</a> </li> <li class="nav-item "> <a class="nav-link" href="/research/">Research</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">Code &amp; Data</a> </li> <li class="nav-item "> <a class="nav-link" href="/posts/">Posts</a> </li> <li class="nav-item "> <a class="nav-link" href="/interns/">Internships</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5"> <div class="publication"> <h1 align="center"> JAFAR: Jack up Any Feature at Any Resolution </h1> <h3 align="center"> <a href="https://scholar.google.fr/citations?user=yQRnP7YAAAAJ&amp;hl=fr" target="_blank" rel="noopener noreferrer">Paul Couairon*</a>   <a href="https://loickch.github.io/" target="_blank" rel="noopener noreferrer">Loick Chambon*</a>    <a href="https://scholar.google.com/citations?user=fKlo-lUAAAAJ&amp;hl=fr" target="_blank" rel="noopener noreferrer">Louis Serrano</a>   <a>Jean-Emmanuel Haugeard</a>   <a href="https://cord.isir.upmc.fr/" target="_blank" rel="noopener noreferrer">Matthieu Cord</a>   <a href="https://thome.isir.upmc.fr" target="_blank" rel="noopener noreferrer">Nicolas Thome</a> </h3> <p align="center"><em>*Equal Contribution</em></p> <h3 align="center"> NeurIPS 2025 </h3> <div class="row justify-content-center"> <div class="column"> <p align="center"> <a href="https://arxiv.org/abs/2506.11136" target="_blank" rel="noopener noreferrer"><i class="far fa-file-pdf"></i> Paper</a>   <a href="https://github.com/PaulCouairon/JAFAR" target="_blank" rel="noopener noreferrer"><i class="fab fa-github"></i> Code</a>    <a href="https://jafar-upsampler.github.io" target="_blank" rel="noopener noreferrer"><i class="fas fa-globe"></i> Website</a>    </p> </div> </div> <hr> <h2 align="center"> Abstract</h2> <p align="justify">Foundation Vision Encoders have become essential for a wide range of dense vision tasks. However, their low-resolution spatial feature outputs necessitate feature upsampling to produce the high-resolution modalities required for downstream tasks. In this work, we introduce JAFAR—a lightweight and flexible feature upsampler that enhances the spatial resolution of visual features from any Foundation Vision Encoder to an arbitrary target resolution. JAFAR employs an attention-based module designed to promote semantic alignment between high-resolution queries—derived from low-level image features—and semantically enriched low-resolution keys, using Spatial Feature Transform (SFT) modulation. Notably, despite the absence of high-resolution supervision, we demonstrate that learning at low upsampling ratios and resolutions generalizes remarkably well to significantly higher output scales. Extensive experiments show that JAFAR effectively recovers fine-grained spatial details and consistently outperforms existing feature upsampling methods across a diverse set of downstream tasks.</p> <hr> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <img src="../../assets/img/publications/2025_jafar/teaser.png" class="img-fluid rounded z-depth-1"> <div class="caption"> JAFAR improves metrics on many downstream tasks: semantic segmentation, depth estimation, feature activation, zero-shot open vocabulary, bird's eye view segmentation by upsampling features from any backbone. </div> </div> </div> <h2 align="center">Results</h2> <p>Once trained, JAFAR can efficiently upsample any backbone features to any resolution. Visually the features are sharper and more detailed than the original features leading to better performance on downstream tasks.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <img src="../../assets/img/publications/2025_jafar/pca.png" class="img-fluid rounded z-depth-1"> <div class="caption"> PCA visualization of features from various upsamplers. </div> </div> </div> <p>Below we present results on various tasks, comparing JAFAR with other upsamplers such as Bilinear, FeatUp, and LiFT. More comparisons can be found in the paper.</p> <h3>Semantic Segmentation</h3> <p>We perform a linear probing on upsampled features from various upsamplers on many datasets: VOC, COCO, ADE20k, and Cityscapes. The results show that JAFAR consistently outperforms other methods across all datasets.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <img src="../../assets/img/publications/2025_jafar/segmentation.png" class="img-fluid rounded z-depth-1"> <div class="caption"> Linear probing results for semantic segmentation across various upsamplers. </div> </div> </div> <table> <thead> <tr> <th>Method</th> <th>COCO mIoU (↑)</th> <th>VOC mIoU (↑)</th> <th>ADE20k mIoU (↑)</th> <th>Cityscapes mIoU (↑)</th> </tr> </thead> <tbody> <tr> <td colspan="5"><strong>Training-Free</strong></td> </tr> <tr> <td>Bilinear</td> <td>59.03</td> <td>80.70</td> <td>39.23</td> <td>59.37</td> </tr> <tr> <td colspan="5"><strong>Task-Agnostic</strong></td> </tr> <tr> <td>FeatUp</td> <td>60.10</td> <td>81.08</td> <td>38.82</td> <td>56.06</td> </tr> <tr> <td>LiFT</td> <td>58.18</td> <td>78.06</td> <td>38.73</td> <td>58.75</td> </tr> <tr> <td><strong>JAFAR (ours)</strong></td> <td> <strong>60.78</strong> <span style="color: green;">(+1.75)</span> </td> <td> <strong>84.44</strong> <span style="color: green;">(+3.74)</span> </td> <td> <strong>40.49</strong> <span style="color: green;">(+1.26)</span> </td> <td> <strong>61.47</strong> <span style="color: green;">(+2.10)</span> </td> </tr> </tbody> </table> <h3>Depth Estimation</h3> <p>For depth estimation, we evaluate the upsampled features using δ₁ and RMSE metrics. JAFAR again shows superior performance compared to other methods.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <img src="../../assets/img/publications/2025_jafar/depth.png" class="img-fluid rounded z-depth-1"> <div class="caption"> Linear probing results for depth estimation across various upsamplers. </div> </div> </div> <table> <thead> <tr> <th>Method</th> <th>δ₁ (↑)</th> <th>RMSE (↓)</th> </tr> </thead> <tbody> <tr> <td colspan="3"><strong>Training-Free</strong></td> </tr> <tr> <td>Bilinear</td> <td>59.92</td> <td>0.66</td> </tr> <tr> <td colspan="3"><strong>Task-Agnostic</strong></td> </tr> <tr> <td>FeatUp</td> <td>61.69</td> <td>0.64</td> </tr> <tr> <td>LiFT</td> <td>57.04</td> <td>0.70</td> </tr> <tr> <td><strong>JAFAR (ours)</strong></td> <td> <strong>62.18</strong> <span style="color: green;">(+2.26)</span> </td> <td> <strong>0.62</strong> <span style="color: green;">(-0.04)</span> </td> </tr> </tbody> </table> <h3>Class Activation Maps</h3> <p>When evaluating Class Activation Maps (CAMs), JAFAR demonstrates improved alignment and granularity in the visualizations, indicating better feature representation.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <img src="../../assets/img/publications/2025_jafar/gradcam.png" class="img-fluid rounded z-depth-1"> <div class="caption"> Class Activation Map visualizations across various upsamplers. </div> </div> </div> <table> <thead> <tr> <th>Method</th> <th>A.D (↓)</th> <th>A.I (↑)</th> <th>A.G (↑)</th> <th>ADCC (↑)</th> </tr> </thead> <tbody> <tr> <td colspan="5"><strong>Training-Free</strong></td> </tr> <tr> <td>Bilinear</td> <td>19.0</td> <td>18.5</td> <td>3.4</td> <td>61.7</td> </tr> <tr> <td colspan="5"><strong>Task-Agnostic</strong></td> </tr> <tr> <td>FeatUp</td> <td><strong>15.3</strong></td> <td>24.0</td> <td>4.3</td> <td>64.3</td> </tr> <tr> <td>LiFT</td> <td>66.9</td> <td>8.7</td> <td>2.3</td> <td>53.0</td> </tr> <tr> <td><strong>JAFAR (ours)</strong></td> <td>17.4 <span style="color: green;">(-1.6)</span> </td> <td> <strong>30.9</strong> <span style="color: green;">(+12.4)</span> </td> <td> <strong>6.5</strong> <span style="color: green;">(+3.1)</span> </td> <td> <strong>73.3</strong> <span style="color: green;">(+11.6)</span> </td> </tr> </tbody> </table> <h3>Vehicle Segmentation</h3> <p>Even on more complicated tasks and pipelines, JAFAR shows significant improvements.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <img src="../../assets/img/publications/2025_jafar/bev.gif" class="img-fluid rounded z-depth-1"> <div class="caption"> Vehicle segmentation in Bird's Eye View using DINOv2 + JAFAR. </div> </div> </div> <table> <thead> <tr> <th>Upsampling</th> <th>SimpleBeV mIoU (↑)</th> <th>PointBeV mIoU (↑)</th> <th>BeVFormer mIoU (↑)</th> </tr> </thead> <tbody> <tr> <td colspan="4"><strong>Training-Free</strong></td> </tr> <tr> <td>Low-Res</td> <td>31.75</td> <td>34.89</td> <td>33.72</td> </tr> <tr> <td>Bilinear</td> <td>33.67</td> <td>36.01</td> <td>34.18</td> </tr> <tr> <td colspan="4"><strong>Task-Agnostic</strong></td> </tr> <tr> <td>FeatUp</td> <td>33.95</td> <td>35.38</td> <td>34.01</td> </tr> <tr> <td><strong>JAFAR (ours)</strong></td> <td> <strong>36.59</strong> <span style="color: green;">(+2.92)</span> </td> <td> <strong>37.20</strong> <span style="color: green;">(+1.19)</span> </td> <td> <strong>36.54</strong> <span style="color: green;">(+2.36)</span> </td> </tr> </tbody> </table> <h3>Zero-shot Open Vocabulary</h3> <p>We also clearly see improvements in zero-shot open vocabulary tasks.</p> <table> <thead> <tr> <th>Upsampling</th> <th>VOC mIoU (↑)</th> <th>ADE mIoU (↑)</th> <th>City mIoU (↑)</th> </tr> </thead> <tbody> <tr> <td colspan="4"><strong>Training-Free</strong></td> </tr> <tr> <td>Bilinear</td> <td>27.87</td> <td>11.03</td> <td>21.56</td> </tr> <tr> <td colspan="4"><strong>Task-Agnostic</strong></td> </tr> <tr> <td>FeatUp</td> <td>32.27</td> <td>13.03</td> <td>24.76</td> </tr> <tr> <td><strong>JAFAR (ours)</strong></td> <td> <strong>35.70</strong> <span style="color: green;">(+7.83)</span> </td> <td> <strong>13.61</strong> <span style="color: green;">(+2.58)</span> </td> <td> <strong>25.26</strong> <span style="color: green;">(+3.70)</span> </td> </tr> </tbody> </table> <hr> <h2 align="center">BibTeX</h2> <left> <pre class="bibtex-box">
@inproceedings{couairon2025jafar,
      title={JAFAR: Jack up Any Feature at Any Resolution}, 
      author={Paul Couairon and Loick Chambon and Louis Serrano and Jean-Emmanuel Haugeard and Matthieu Cord and Nicolas Thome},
      year={2025},
      url={https://jafar-upsampler.github.io}, 
      booktitle={NeurIPS}, 
}
</pre> </left> <p><br></p> </div> </div> <footer class="sticky-bottom mt-5"> <div class="container">valeo.ai research page </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id="></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","");</script> </body> </html>