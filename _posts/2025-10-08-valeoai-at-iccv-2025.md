---
toc: false
layout: post
description: "Sophia Sirko-Galouchenko, LoÃ¯ck Chambon, Shashanka Venkataramanan, Tuan-Hung Vu, Andrei Bursuc, Eloi Zablocki"
categories: [multi-sensor, 3d-perception, foundation, limited-supervision, zero shot, deep-learning, generalization, explainability]
title: "valeo.ai at ICCV 2025"
permalink: /posts/iccv-2025
hide: false
image: assets/img/posts/ICCV_2025.png
---

The [International Conference on Computer Vision (ICCV)](https://iccv.thecvf.com/) is a leading conference that brings together researchers and practitioners in computer vision and machine learning. At the 2025 edition, the [valeo.ai](../) team will present five papers in the main conference. We are also contributing to the [Foundational Data for Industrial Tech Transfer](https://iccv2025-found-workshop.limitlab.xyz) workshop with a keynote on [Towards openness of vision foundation models](https://iccv2025-found-workshop.limitlab.xyz/program).  
The team will be at ICCV to present these works, exchange ideas, and share our exciting ongoing research. We look forward to seeing you in Honolulu!

![valeo.ai team at ICCV 2025]({{ site.baseurl }}/assets/img/posts/valeoai_iccv.png){:height="100%" width="100%"}

<hr>

## DIP: Unsupervised Dense In-Context Post-training of Visual Representations
### Authors: <a href="https://scholar.google.com/citations?user=3ac3PQMAAAAJ&hl=fr">Sophia Sirko-Galouchenko</a> Â Â  <a href="https://scholar.google.fr/citations?user=7atfg7EAAAAJ&hl=en">Spyros Gidaris</a> Â Â  <a href="https://vobecant.github.io/">Antonin Vobecky</a> Â Â  <a href="https://abursuc.github.io/">Andrei Bursuc</a> Â Â  <a href="https://scholar.google.com/citations?user=3ac3PQMAAAAJ&hl=fr">Nicolas Thome</a>
<h4 align="center"> [<a href="https://arxiv.org/abs/2506.18463">Paper</a>] &nbsp;&nbsp; [<a href="https://github.com/sirkosophia/DIP">Code</a>] </h4>

![dip_teaser]({{ site.baseurl }}/assets/img/publications/2025_dip.png){:height="100%" width="100%"}

We introduce DIP, a novel unsupervised post-training method designed to enhance dense image representations in large-scale pretrained vision encoders for in-context scene understanding. Unlike prior approaches that rely on complex self-distillation architectures, our method trains the vision encoder using pseudo-tasks that explicitly simulate downstream in-context scenarios, inspired by meta-learning principles. To enable post-training on unlabeled data, we propose an automatic mechanism for generating in-context tasks that combines a pretrained diffusion model and the vision encoder itself. DIP is simple, unsupervised, and computationally efficient, requiring less than 9 hours on a single A100 GPU. By learning dense representations through pseudo in-context tasks, it achieves strong performance across a wide variety of downstream real-world in-context scene understanding tasks. It outperforms both the initial vision encoder and prior methods, offering a practical and effective solution for improving dense representations.

<hr>

## GaussRender: Learning 3D Occupancy with Gaussian Rendering
### Authors: <a href="https://loickch.github.io/">LoÃ¯ck Chambon</a> Â Â  <a href="https://eloiz.github.io">Ã‰loi Zablocki</a> Â Â  <a href="https://boulch.eu/">Alexandre Boulch</a> Â Â  <a href="https://scholar.google.com/citations?user=QnRpMJAAAAAJ">MickaÃ«l Chen</a> Â Â  <a href="https://cord.isir.upmc.fr/">Matthieu Cord</a>
<h4 align="center"> [<a href="https://arxiv.org/abs/2502.05040">Paper</a>] &nbsp;&nbsp; [<a href="https://github.com/valeoai/GaussRender">Code</a>] </h4>

![gaussrender_teaser]({{ site.baseurl }}/assets/img/publications/2025_gaussrender/teaser.png){:height="100%" width="100%"}

Understanding the 3D geometry and semantics of driving scenes is critical for safe autonomous driving. Recent advances in 3D occupancy prediction improve scene representation but often suffer from spatial inconsistencies, leading to floating artifacts and poor surface localization. Existing voxel-wise losses (e.g., cross-entropy) fail to enforce geometric coherence. GaussRender is a module that improves 3D occupancy learning by enforcing projective consistency. The key idea is to project both predicted and ground-truth 3D occupancy into 2D camera views for supervision, penalizing inconsistent 3D configurations and enforcing coherent 3D structure. To achieve this efficiently, GaussRender leverages differentiable rendering with Gaussian splatting. It integrates seamlessly with existing architectures, requires no inference-time modifications, and significantly improves geometric fidelity across multiple benchmarks (SurroundOcc-nuScenes, Occ3D-nuScenes, SSCBench-KITTI360) and 3D occupancy models (TPVFormer, SurroundOcc, Symphonies).

<hr> <div class="row"> <div class="col-sm mt-3 mt-md-0" style="padding-right: 5px;padding-left: 5px;"> <img src="../../assets/img/publications/2025_gaussrender/demo_scene_0003.gif" class="img-fluid rounded z-depth-1"/> <div class="caption"> Scene visualization 1 </div> </div> <div class="col-sm mt-3 mt-md-0" style="padding-right: 5px;padding-left: 5px;"> <img src="../../assets/img/publications/2025_gaussrender/demo_scene_0013.gif" class="img-fluid rounded z-depth-1"/> <div class="caption"> Scene visualization 2 </div> </div> </div> <div class="caption"> GaussRender is a 3D Occupancy module that can be plugged into any 3D Occupancy model to enhance its predictions and ensure 2D-3D consistency while improving mIoU, IoU, and RayIoU. </div> <hr> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <img src="../../assets/img/publications/2025_gaussrender/pipeline.png" class="img-fluid rounded z-depth-1"/> <div class="caption"> GaussRender can be plugged into any model. The core idea is to transform voxels into Gaussians before performing a depth and semantic rendering. </div> </div> </div> <h2 align="center">Results</h2>

GaussRender can be plugged into any 3D model. Dedicated experiments on multiple 3D benchmarks (SurroundOcc-nuScenes, Occ3D-nuScenes, SSCBench-KITTI360) and models (TPVFormer, SurroundOcc, Symphonies) demonstrate its performance.

<h3>Occ3D-nuScenes</h3> <table> <thead> <tr> <th>Models</th> <th><a href="https://arxiv.org/abs/2502.05040">TPVFormer (ours)</a></th> <th><a href="https://arxiv.org/abs/2302.07817">TPVFormer</a></th> <th><a href="https://arxiv.org/abs/2502.05040">SurroundOcc (ours)</a></th> <th><a href="https://arxiv.org/abs/2303.09551">SurroundOcc</a></th> <th><a href="https://arxiv.org/abs/2304.05316">OccFormer</a></th> <th><a href="https://arxiv.org/abs/2309.09502">RenderOcc</a></th> </tr> <tr> <th>Type</th> <th>w/ GaussRender</th> <th>base</th> <th>w/ GaussRender</th> <th>base</th> <th>base</th> <th>base</th> </tr> </thead> <tbody> <tr> <td>mIoU</td> <td><strong>30.48 ðŸ¥‡</strong> <span style="color: green;">(+2.65)</span></td> <td>27.83</td> <td>30.38 ðŸ¥ˆ <span style="color: green;">(+1.17)</span></td> <td>29.21</td> <td>21.93</td> <td>26.11</td> </tr> <tr> <td>RayIoU</td> <td><strong>38.3 ðŸ¥‡</strong> <span style="color: green;">(+1.1)</span></td> <td>37.2</td> <td>37.5 ðŸ¥ˆ <span style="color: green;">(+2.0)</span></td> <td>35.5</td> <td>-</td> <td>19.5</td> </tr> </tbody> </table> <h3>SurroundOcc-nuScenes</h3> <table> <thead> <tr> <th>Models</th> <th><a href="https://arxiv.org/abs/2502.05040">TPVFormer (ours)</a></th> <th><a href="https://arxiv.org/abs/2302.07817">TPVFormer</a></th> <th><a href="https://arxiv.org/abs/2502.05040">SurroundOcc (ours)</a></th> <th><a href="https://arxiv.org/abs/2303.09551">SurroundOcc</a></th> <th><a href="https://arxiv.org/abs/2304.05316">OccFormer</a></th> <th><a href="https://arxiv.org/abs/2412.04384">GaussianFormerv2</a></th> </tr> <tr> <th>Type</th> <th>w/ GaussRender</th> <th>base</th> <th>w/ GaussRender</th> <th>base</th> <th>base</th> <th>base</th> </tr> </thead> <tbody> <tr> <td>IoU</td> <td>32.05 ðŸ¥ˆ <span style="color: green;">(+1.19)</span></td> <td>30.86</td> <td><strong>32.61 ðŸ¥‡</strong> <span style="color: green;">(+1.12)</span></td> <td>31.49</td> <td>31.39</td> <td>30.56</td> </tr> <tr> <td>mIoU</td> <td>20.58 ðŸ¥ˆ <span style="color: green;">(+3.48)</span></td> <td>17.10</td> <td><strong>20.82 ðŸ¥‡</strong> <span style="color: green;">(+0.52)</span></td> <td>20.30</td> <td>19.03</td> <td>20.02</td> </tr> </tbody> </table> <h3>SSCBench-KITTI360</h3> <table> <thead> <tr> <th>Models</th> <th><a href="https://arxiv.org/abs/2502.05040">SurroundOcc (ours)</a></th> <th><a href="https://arxiv.org/abs/2303.09551">SurroundOcc</a></th> <th><a href="https://arxiv.org/abs/2502.05040">Symphonies (ours)</a></th> <th><a href="https://arxiv.org/abs/2306.15670">Symphonies</a></th> <th><a href="https://arxiv.org/abs/2304.05316">OccFormer</a></th> <th><a href="https://arxiv.org/abs/2112.00726">MonoScene</a></th> </tr> <tr> <th>Type</th> <th>w/ GaussRender</th> <th>base</th> <th>w/ GaussRender</th> <th>base</th> <th>base</th> <th>base</th> </tr> </thead> <tbody> <tr> <td>IoU</td> <td>38.62 <span style="color: green;">(+0.11)</span></td> <td>38.51</td> <td><strong>44.08 ðŸ¥‡</strong> <span style="color: green;">(+0.68)</span></td> <td>43.40 ðŸ¥ˆ</td> <td>40.27</td> <td>37.87</td> </tr> <tr> <td>mIoU</td> <td>13.34 <span style="color: green;">(+0.26)</span></td> <td>13.08</td> <td><strong>18.11 ðŸ¥‡</strong> <span style="color: green;">(+0.29)</span></td> <td>17.82 ðŸ¥ˆ</td> <td>13.81</td> <td>12.31</td> </tr> </tbody> </table>

<hr>

## MoSiC: Optimal-Transport Motion Trajectories for Dense Self-Supervised Learning
### Authors: <a href="https://scholar.google.com/citations?user=kpT3gcsAAAAJ&hl=en">Mohammadreza Salehi</a> Â Â  <a href="https://shashankvkt.github.io/">Shashanka Venkataramanan</a> Â Â  Ioana Simion Â Â  <a href="https://www.egavves.com/">Efstratios Gavves</a> Â Â  <a href="https://www.ceessnoek.info/">Cees Snoek</a> Â Â  <a href="https://yukimasano.github.io/">Yuki Asano</a>
<h4 align="center"> [<a href="https://arxiv.org/abs/2506.08694">Paper</a>] &nbsp;&nbsp; [<a href="https://github.com/SMSD75/MoSiC/tree/main">Code</a>] </h4>

![mosic_teaser]({{ site.baseurl }}/assets/img/publications/2025_mosic.png){:height="100%" width="100%"}

Dense self-supervised learning has shown great promise for learning pixel- and patch-level representations, but extending it to videos remains challenging due to complex motion dynamics. Existing approaches struggle under object deformations, occlusions, and camera movement, leading to inconsistent feature learning over time. In this work, we introduce MoSiC, a motion-guided self-supervised framework that clusters dense point tracks to learn spatiotemporally consistent representations. Using an off-the-shelf point tracker, we extract long-range motion trajectories and optimize feature clustering with a momentum-encoder-based optimal transport mechanism. Temporal coherence is enforced by propagating cluster assignments along tracked points, ensuring feature consistency across views despite viewpoint changes. By leveraging motion as an implicit supervisory signal and initializing from strong image-pretrained models, MoSiC learns robust representations that generalize across frames. Our approach improves state-of-the-art performance by 1% to 6% across six image and video datasets and four evaluation benchmarks.

<hr>

## FLOSS: Free Lunch in Open-vocabulary Semantic Segmentation
### Authors: <a href="https://yasserben.github.io/">Yasser Benigmim</a> Â Â  <a href="https://mfahes.github.io/">Mohammad Fahes</a> Â Â  <a href="https://tuanhungvu.github.io/">Tuan-Hung Vu</a> Â Â  <a href="https://abursuc.github.io/">Andrei Bursuc</a> Â Â  <a href="https://team.inria.fr/rits/membres/raoul-de-charette/">Raoul de Charette</a>
<h4 align="center"> [<a href="https://arxiv.org/abs/2504.10487">Paper</a>] &nbsp;&nbsp; [<a href="https://github.com/yasserben/FLOSS">Code</a>] </h4>

![floss_teaser]({{ site.baseurl }}/assets/img/publications/2025_floss.jpg){:height="100%" width="100%"}

In Open-Vocabulary Semantic Segmentation (OVSS), class-wise text embeddings are usually averaged over multiple templates (e.g., "a photo of <class>", "a sketch of <class>") to form classifiers. We show that for each class, there exist single-template classifiersâ€”termed class-expertsâ€”that outperform conventional averaged classifiers. To identify these class-experts without labeled data, we leverage class-wise prediction entropy and select the lowest-entropy classifiers as the most reliable. We then fuse the outputs of these class-experts using a novel plug-and-play process called FLOSS. FLOSS is complementary to existing OVSS methods, requiring no additional labels or training, yet consistently improves performance. Extensive experiments demonstrate that FLOSS enhances state-of-the-art OVSS models, generalizes across datasets with distribution shifts, and yields strong improvements in low-data scenarios with only a few unlabeled images.

<hr>

## Analyzing Fine-tuning Representation Shift for Multimodal LLMs Steering Alignment
### Authors: <a href="https://pegah-kh.github.io/">Pegah Khayatan</a> Â Â  <a href="https://mustafashukor.github.io/">Mustafa Shukor</a> Â Â  <a href="https://jayneelparekh.github.io/">Jayneel Parekh</a> Â Â  <a href="https://cord.isir.upmc.fr/">Matthieu Cord</a>
<h4 align="center"> [<a href="https://arxiv.org/abs/2501.03012">Paper</a>] &nbsp;&nbsp; [<a href="https://github.com/mshukor/xl-vlms/">Code</a>] </h4>

![xlvlm_teaser]({{ site.baseurl }}/assets/img/publications/2025_xlxvlm.png){:height="100%" width="100%"}

Multimodal LLMs have achieved remarkable proficiency in understanding multimodal inputs, yet little attention has been given to explaining how their internal representations evolve during training. Most explainability research focuses only on final model states, ignoring dynamic representational shifts. In this work, we systematically analyze the evolution of hidden state representations during fine-tuning, revealing how models adapt to new multimodal tasks. Using a concept-based approach, we map hidden states to interpretable visual and textual concepts, enabling us to trace concept changes across modalities as training progresses. We introduce shift vectors to capture these changes, allowing recovery of fine-tuned concepts from the original model. Furthermore, we demonstrate practical applications in model steering, such as adjusting answer types, caption styles, or biasing responses without additional training. This work provides novel insights into multimodal representation adaptation and offers tools for interpreting and controlling fine-tuned multimodal LLMs.
