<!DOCTYPE html> <html lang="en"> <head> <meta name="google-site-verification" content=""/> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>valeo.ai at NeurIPS 2024 | valeo.ai - valeo.ai research page</title> <meta name="author" content=" "/> <meta name="description" content="Victor Letzelter, Nermin Samet, Yuan Yin, Andrei Bursuc, Éloi Zablocki"/> <meta name="keywords" content="computer vision, ai, valeo, artificial intelligence, research, deep learning"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light"/> <link rel="shortcut icon" href="/assets/img/valeoai_logo_256x256.png"/> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://valeoai.github.io//posts/2024-12-04-valeoai-at-neurips-2024/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous"> <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"> </script> <script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"></script> <script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"[%",right:"%]",display:!0},{left:"$",right:"$",display:!1}]})});</script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><img src="/assets/img/valeoai_logo.png" alt="valeo.ai" class="title-logo" height="24px"></a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/"></a> </li> <li class="nav-item "> <a class="nav-link" href="/team/">Team</a> </li> <li class="nav-item "> <a class="nav-link" href="/research/">Research</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">Code &amp; Data</a> </li> <li class="nav-item "> <a class="nav-link" href="/posts/">Posts</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">valeo.ai at NeurIPS 2024</h1> <p class="post-meta">December 4, 2024 • <span class="read-time" title="Estimated read time"> 7 min read </span></p> <p class="post-tags"> <a href="2024"> <i class="fas fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/category/3d%20perception"> <i class="fas fa-tag fa-sm"></i> 3d perception</a>   <a href="/category/multi-sensor"> <i class="fas fa-tag fa-sm"></i> multi-sensor</a>   <a href="/category/limited%20supervision"> <i class="fas fa-tag fa-sm"></i> limited supervision</a>   <a href="/category/reliability"> <i class="fas fa-tag fa-sm"></i> reliability</a>   <a href="/category/motion%20forecasting"> <i class="fas fa-tag fa-sm"></i> motion forecasting</a>   <a href="/category/robustness"> <i class="fas fa-tag fa-sm"></i> robustness</a>   <a href="/category/generalization"> <i class="fas fa-tag fa-sm"></i> generalization</a>   <a href="/category/driving"> <i class="fas fa-tag fa-sm"></i> driving</a>   <a href="/category/deep-learning"> <i class="fas fa-tag fa-sm"></i> deep-learning</a>   </p> </header> <article class="post-content"> <p>The <a href="https://neurips.cc/" target="_blank" rel="noopener noreferrer">Neural Information Processing Systems Conference (NeurIPS)</a> is a major inter-disciplinary event that brings together researchers and practicioners in machine learning, computer vision, natural language processing, optimization, statistics, but also neuroscience, natural sciences, social sciences, etc. This year, at the thirty-eigth edition of NeurIPS, the <a href="../">valeo.ai</a> team will present 7 papers in the main conference.</p> <p>We will be happy to discuss more about these projects and ideas, and share our exciting ongoing research. Take a quick view of our papers below and come meet us at the posters or catch us for a coffee in the hallways.</p> <hr> <h2 id="no-train-all-gain-self-supervised-gradients-improve-deep-frozen-representations">No Train, all Gain: Self-Supervised Gradients Improve Deep Frozen Representations</h2> <h3 id="authors-walter-simoncini---spyros-gidaris----andrei-bursuc--yuki-m-asano">Authors: <a href="https://walter.ashita.nl/" target="_blank" rel="noopener noreferrer">Walter Simoncini</a>    <a href="https://scholar.google.fr/citations?user=7atfg7EAAAAJ&amp;hl=en" target="_blank" rel="noopener noreferrer">Spyros Gidaris</a>    <a href="https://abursuc.github.io/" target="_blank" rel="noopener noreferrer">Andrei Bursuc</a>    <a href="https://yukimasano.github.io/" target="_blank" rel="noopener noreferrer">Yuki M. Asano</a> </h3> <h4 align="center"> [<a href="https://arxiv.org/abs/2407.10964" target="_blank" rel="noopener noreferrer">Paper</a>]    [<a href="https://github.com/WalterSimoncini/fungivision" target="_blank" rel="noopener noreferrer">Code</a>]    [<a href="https://valeoai.github.io/publications/fungi">Project page</a>]</h4> <p><img src="/assets/img/publications/2024_fungi/overview.jpg" alt="fungi_overview" height="100%" width="100%"></p> <p>This paper introduces FUNGI: Features from UNsupervised GradIents, a method to enhance the features of vision encoders by leveraging self-supervised gradients. Our method is simple: given any pretrained model, we first compute gradients from various self-supervised objectives for each input. These are projected to a lower dimension and then concatenated with the model’s embedding. The resulting features are evaluated on k-nearest neighbor classification over 11 datasets from vision, 5 from natural language processing, and 2 from audio. Across backbones spanning various sizes and pretraining strategies, FUNGI features provide consistent performance improvements over the embeddings. We also show that using FUNGI features can benefit linear classification and image retrieval, and that they significantly improve the retrieval-based in-context scene understanding abilities of pretrained models, for example improving upon DINO by +17% for semantic segmentation — without any training.</p> <hr> <h2 id="manipose-manifold-constrained-multi-hypothesis-3d-human-pose-estimation">ManiPose: Manifold-Constrained Multi-Hypothesis 3D Human Pose Estimation</h2> <h3 id="authors-cédric-rommel--victor-letzelter--nermin-samet--renaud-marlet---matthieu-cord--patrick-pérez--eduardo-valle">Authors: <a href="https://cedricrommel.github.io/" target="_blank" rel="noopener noreferrer">Cédric Rommel</a>    <a href="https://scholar.google.com/citations?user=YhTdZh8AAAAJ&amp;hl=en&amp;oi=ao" target="_blank" rel="noopener noreferrer">Victor Letzelter</a>    <a href="https://nerminsamet.github.io/" target="_blank" rel="noopener noreferrer">Nermin Samet</a>    <a href="http://imagine.enpc.fr/~marletr/" target="_blank" rel="noopener noreferrer">Renaud Marlet</a>    <a href="https://cord.isir.upmc.fr/" target="_blank" rel="noopener noreferrer">Matthieu Cord</a>    <a href="https://ptrckprz.github.io/" target="_blank" rel="noopener noreferrer">Patrick Pérez</a>    <a href="https://eduardovalle.com/" target="_blank" rel="noopener noreferrer">Eduardo Valle</a> </h3> <h4 align="center"> [<a href="https://arxiv.org/abs/2312.06386" target="_blank" rel="noopener noreferrer">Paper</a>]    [<a href="https://github.com/cedricrommel/manipose" target="_blank" rel="noopener noreferrer">Code</a>]    [<a href="https://valeoai.github.io/publications/manipose/">Project page</a>]</h4> <p>We propose ManiPose, a manifold-constrained multi-hypothesis model for human-pose 2D-to-3D lifting. We provide theoretical and empirical evidence that, due to the depth ambiguity inherent to monocular 3D human pose estimation, traditional regression models suffer from pose-topology consistency issues, which standard evaluation metrics (MPJPE, P-MPJPE and PCK) fail to assess. ManiPose addresses depth ambiguity by proposing multiple candidate 3D poses for each 2D input, each with its estimated plausibility. Unlike previous multi-hypothesis approaches, ManiPose forgoes generative models, greatly facilitating its training and usage. By constraining the outputs to lie on the human pose manifold, ManiPose guarantees the consistency of all hypothetical poses, in contrast to previous works. We showcase the performance of ManiPose on real-world datasets, where it outperforms state-of-the-art models in pose consistency by a large margin while being very competitive on the MPJPE metric.</p> <p><img src="/assets/img/publications/2024_manipose/ManiPose_teaser.png" alt="manipose_overview" height="90%" width="90%"></p> <hr> <h2 id="annealed-multiple-choice-learning-overcoming-limitations-of-winner-takes-all-with-annealing">Annealed Multiple Choice Learning: Overcoming limitations of Winner-takes-all with annealing</h2> <h3 id="authors-david-perera--victor-letzelter--théo-mariotte---adrien-cortés--mickael-chen--slim-essid--gaël-richard">Authors: <a href="https://adasp.telecom-paris.fr/members/" target="_blank" rel="noopener noreferrer">David Perera</a>    <a href="https://scholar.google.com/citations?user=YhTdZh8AAAAJ&amp;hl=en&amp;oi=ao" target="_blank" rel="noopener noreferrer">Victor Letzelter</a>    <a href="https://scholar.google.com/citations?user=q3HZFcwAAAAJ" target="_blank" rel="noopener noreferrer">Théo Mariotte </a>    <a href="https://www.linkedin.com/in/c1adrien/" target="_blank" rel="noopener noreferrer">Adrien Cortés</a>    <a href="https://www.linkedin.com/in/mickael-chen-ml/" target="_blank" rel="noopener noreferrer">Mickael Chen</a>    <a href="https://slimessid.github.io/research/" target="_blank" rel="noopener noreferrer">Slim Essid</a>    <a href="https://www.telecom-paris.fr/gael-richard" target="_blank" rel="noopener noreferrer">Gaël Richard</a> </h3> <h4 align="center"> [<a href="https://arxiv.org/abs/2407.15580" target="_blank" rel="noopener noreferrer">Paper</a>]    [<a href="https://github.com/Victorletzelter/annealed_mcl" target="_blank" rel="noopener noreferrer">Code</a>]    [<a href="https://valeoai.github.io/publications/annealing/">Project page</a>]</h4> <p>We introduce Annealed Multiple Choice Learning (aMCL) which combines simulated annealing with MCL. MCL is a learning framework handling ambiguous tasks by predicting a small set of plausible hypotheses. These hypotheses are trained using the Winner-takes-all (WTA) scheme, which promotes the diversity of the predictions. However, this scheme may converge toward an arbitrarily suboptimal local minimum, due to the greedy nature of WTA. We overcome this limitation using annealing, which enhances the exploration of the hypothesis space during training. We leverage insights from statistical physics and information theory to provide a detailed description of the model training trajectory. Additionally, we validate our algorithm by extensive experiments on synthetic datasets, on the standard UCI benchmark, and on speech separation.</p> <p><img src="/assets/img/publications/2024_annealing/amcl_gif.gif" alt="annealing_overview" height="100%" width="100%"></p> <hr> <h2 id="geps-boosting-generalization-in-parametric-pde-neural-solvers-through-adaptive-conditioning">GEPS: Boosting Generalization in Parametric PDE Neural Solvers through Adaptive Conditioning</h2> <h3 id="authors-armand-kassaï-koupaï--jorge-mifsut-benet--yuan-yin--jean-noël-vittaut--patrick-gallinari">Authors: <a href="https://itsakk.github.io/" target="_blank" rel="noopener noreferrer">Armand Kassaï Koupaï</a>    <a href="https://www.isir.upmc.fr/personnel/mifsutbenet/" target="_blank" rel="noopener noreferrer">Jorge Mifsut-Benet</a>    <a href="https://yuan-yin.github.io" target="_blank" rel="noopener noreferrer">Yuan Yin</a>    <a href="https://webia.lip6.fr/~vittaut/" target="_blank" rel="noopener noreferrer">Jean-Noël Vittaut</a>    <a href="https://pages.isir.upmc.fr/gallinari/" target="_blank" rel="noopener noreferrer">Patrick Gallinari</a> </h3> <h4 align="center"> [<a href="https://arxiv.org/abs/2410.23889" target="_blank" rel="noopener noreferrer">Paper</a>]    [<a href="https://github.com/itsakk/geps" target="_blank" rel="noopener noreferrer">Code</a>]    [<a href="https://valeoai.github.io/publications/geps/">Project page</a>]</h4> <p>Solving parametric partial differential equations (PDEs) presents significant challenges for data-driven methods due to the sensitivity of spatio-temporal dynamics to variations in PDE parameters. Machine learning approaches often struggle to capture this variability. To address this, data-driven approaches learn parametric PDEs by sampling a very large variety of trajectories with varying PDE parameters. We first show that incorporating conditioning mechanisms for learning parametric PDEs is essential and that among them, \textit{adaptive conditioning}, allows stronger generalization. As existing adaptive conditioning methods do not scale well with respect to the number of PDE parameters, we propose GEPS, a simple adaptation mechanism to boost GEneralization in Pde Solvers via a first-order optimization and low-rank rapid adaptation of a small set of context parameters. We demonstrate the versatility of our approach for both fully data-driven and for physics-aware neural solvers. Validation performed on a whole range of spatio-temporal forecasting problems demonstrates excellent performance for generalizing to unseen conditions including initial conditions, PDE coefficients, forcing terms and solution domain.</p> <p><img src="/assets/img/publications/2024_geps/geps.png" alt="geps_example" height="100%" width="100%"></p> <hr> <h2 id="a-concept-based-explainability-framework-for-large-multimodal-models">A Concept-Based Explainability Framework for Large Multimodal Models</h2> <h3 id="authors-jayneel-parekh--pegah-khayatan--mustafa-shukor--alasdair-newson--matthieu-cord">Authors: <a href="https://jayneelparekh.github.io/" target="_blank" rel="noopener noreferrer">Jayneel Parekh</a>    <a href="https://pegah-kh.github.io/" target="_blank" rel="noopener noreferrer">Pegah Khayatan</a>    <a href="https://mustafashukor.github.io/" target="_blank" rel="noopener noreferrer">Mustafa Shukor</a>    <a href="https://sites.google.com/site/alasdairnewson/" target="_blank" rel="noopener noreferrer">Alasdair Newson</a>    <a href="https://cord.isir.upmc.fr/" target="_blank" rel="noopener noreferrer">Matthieu Cord</a> </h3> <h4 align="center"> [<a href="https://arxiv.org/abs/2406.08074" target="_blank" rel="noopener noreferrer">Paper</a>]    [<a href="https://github.com/mshukor/xl-vlms" target="_blank" rel="noopener noreferrer">Code</a>]    [<a href="https://jayneelparekh.github.io/LMM_Concept_Explainability/" target="_blank" rel="noopener noreferrer">Project page</a>]</h4> <p>Large multimodal models (LMMs) combine unimodal encoders and large language models (LLMs) to perform multimodal tasks. Despite recent advancements towards the interpretability of these models, understanding internal representations of LMMs remains largely a mystery. In this paper, we present a novel framework for the interpretation of LMMs. We propose a dictionary learning based approach, applied to the representation of tokens. The elements of the learned dictionary correspond to our proposed concepts. We show that these concepts are well semantically grounded in both vision and text. Thus we refer to these as “multi-modal concepts”. We qualitatively and quantitatively evaluate the results of the learnt concepts. We show that the extracted multimodal concepts are useful to interpret representations of test samples. Finally, we evaluate the disentanglement between different concepts and the quality of grounding concepts visually and textually.</p> <p><img src="/assets/img/publications/2024_neurips/concept_xai.PNG" alt="concept_overview" height="100%" width="100%"></p> <hr> <h2 id="diffcut-catalyzing-zero-shot-semantic-segmentation-with-diffusion-features-and-recursive-normalized-cut">DiffCut: Catalyzing Zero-Shot Semantic Segmentation with Diffusion Features and Recursive Normalized Cut</h2> <h3 id="authors-paul-couairon--mustafa-shukor--jean-emmanuel-haugeard--matthieu-cord--nicolas-thomea">Authors: <a href="https://scholar.google.fr/citations?user=yQRnP7YAAAAJ" target="_blank" rel="noopener noreferrer">Paul Couairon</a>    <a href="https://mustafashukor.github.io/" target="_blank" rel="noopener noreferrer">Mustafa Shukor</a>    <a href="https://dblp.org/pid/92/6849.html" target="_blank" rel="noopener noreferrer">Jean-Emmanuel Haugeard</a>    <a href="https://cord.isir.upmc.fr/" target="_blank" rel="noopener noreferrer">Matthieu Cord</a>    <a href="https://thome.isir.upmc.fr/" target="_blank" rel="noopener noreferrer">Nicolas Thome/a&gt;</a> </h3> <h4 align="center"> [<a href="https://arxiv.org/abs/2406.02842" target="_blank" rel="noopener noreferrer">Paper</a>]    [<a href="https://github.com/PaulCouairon/DiffCut" target="_blank" rel="noopener noreferrer">Code</a>]    [<a href="https://diffcut-segmentation.github.io/" target="_blank" rel="noopener noreferrer">Project page</a>]</h4> <p>Foundation models have emerged as powerful tools across various domains including language, vision, and multimodal tasks. While prior works have addressed unsupervised image segmentation, they significantly lag behind supervised models. In this paper, we use a diffusion UNet encoder as a foundation vision encoder and introduce DiffCut, an unsupervised zero-shot segmentation method that solely harnesses the output features from the final self-attention block. Through extensive experimentation, we demonstrate that the utilization of these diffusion features in a graph based segmentation algorithm, significantly outperforms previous state-of-the-art methods on zero-shot segmentation. Specifically, we leverage a recursive Normalized Cut algorithm that softly regulates the granularity of detected objects and produces well-defined segmentation maps that precisely capture intricate image details. Our work highlights the remarkably accurate semantic knowledge embedded within diffusion UNet encoders that could then serve as foundation vision encoders for downstream tasks.</p> <p><img src="/assets/img/publications/2024_neurips/diffcut.PNG" alt="diffcut_overview" height="100%" width="100%"></p> <hr> <h2 id="implicit-multimodal-alignment-on-the-generalization-of-frozen-llms-to-multimodal-inputs">Implicit Multimodal Alignment: On the Generalization of Frozen LLMs to Multimodal Inputs</h2> <h3 id="authors-mustafa-shukor--matthieu-cord">Authors: <a href="https://mustafashukor.github.io/" target="_blank" rel="noopener noreferrer">Mustafa Shukor</a>    <a href="https://cord.isir.upmc.fr/" target="_blank" rel="noopener noreferrer">Matthieu Cord</a> </h3> <h4 align="center"> [<a href="https://arxiv.org/abs/2405.16700" target="_blank" rel="noopener noreferrer">Paper</a>]    [<a href="https://github.com/mshukor/ima-lmms" target="_blank" rel="noopener noreferrer">Code</a>]    [<a href="https://ima-lmms.github.io/" target="_blank" rel="noopener noreferrer">Project page</a>]</h4> <p>Large Language Models (LLMs) have demonstrated impressive performance on multimodal tasks, without any multimodal finetuning. They are the building block for Large Multimodal Models, yet, we still lack a proper understanding of their success. In this work, we expose frozen LLMs to image, video, audio and text inputs and analyse their internal representation aiming to understand their generalization beyond textual inputs. Findings. Perceptual tokens (1) are easily distinguishable from textual ones inside LLMs, with significantly different representations, and complete translation to textual tokens does not exist. Yet, (2) both perceptual and textual tokens activate similar LLM weights. Despite being different, (3) perceptual and textual tokens are implicitly aligned inside LLMs, we call this the implicit multimodal alignment (IMA), and argue that this is linked to architectural design, helping LLMs to generalize. This provide more evidence to believe that the generalization of LLMs to multimodal inputs is mainly due to their architecture. Implications. (1) We find a positive correlation between the implicit alignment score and the task performance, suggesting that this could act as a proxy metric for model evaluation and selection. (2) A negative correlation exists regarding hallucinations, revealing that this problem is mainly due to misalignment between the internal perceptual and textual representations. (3) Perceptual tokens change slightly throughout the model, thus, we propose different approaches to skip computations (e.g. in FFN layers), and significantly reduce the inference cost. (4) Due to the slowly changing embeddings across layers, and the high overlap between textual and multimodal activated weights, we compress LLMs by keeping only 1 subnetwork that works well across a wide range of multimodal tasks.</p> <p><img src="/assets/img/publications/2024_neurips/ima_llm.PNG" alt="implicit_overview" height="100%" width="100%"></p> <hr> </article> </div> </div> <footer class="sticky-bottom mt-5"> <div class="container">valeo.ai research page </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id="></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","");</script> </body> </html>