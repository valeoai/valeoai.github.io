<!DOCTYPE html> <html lang="en"> <head> <meta name="google-site-verification" content=""/> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>valeo.ai at NeurIPS 2023 | valeo.ai - valeo.ai research page</title> <meta name="author" content=" "/> <meta name="description" content="Victor Letzelter, Antonin Vobecky, Mickael Chen, Cedric Rommel, Matthieu Cord, Andrei Bursuc"/> <meta name="keywords" content="computer vision, ai, valeo, artificial intelligence, research, deep learning"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light"/> <link rel="shortcut icon" href="/assets/img/valeoai_logo_256x256.png"/> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://valeoai.github.io//posts/neurips-2023"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous"> <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"> </script> <script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"></script> <script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"[%",right:"%]",display:!0},{left:"$",right:"$",display:!1}]})});</script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><img src="/assets/img/valeoai_logo.png" alt="valeo.ai" class="title-logo" height="24px"></a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/"></a> </li> <li class="nav-item "> <a class="nav-link" href="/team/">Team</a> </li> <li class="nav-item "> <a class="nav-link" href="/research/">Research</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">Code &amp; Data</a> </li> <li class="nav-item "> <a class="nav-link" href="/posts/">Posts</a> </li> <li class="nav-item "> <a class="nav-link" href="/interns/">Internships</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">valeo.ai at NeurIPS 2023</h1> <p class="post-meta">December 8, 2023 • <span class="read-time" title="Estimated read time"> 6 min read </span></p> <p class="post-tags"> <a href="2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a>   ·   <a href="/category/multi-sensor"> <i class="fas fa-tag fa-sm"></i> multi-sensor</a>   <a href="/category/limited%20supervision"> <i class="fas fa-tag fa-sm"></i> limited supervision</a>   <a href="/category/reliability"> <i class="fas fa-tag fa-sm"></i> reliability</a>   <a href="/category/deep-learning"> <i class="fas fa-tag fa-sm"></i> deep-learning</a>   </p> </header> <article class="post-content"> <p>The <a href="https://neurips.cc/" target="_blank" rel="noopener noreferrer">Neural Information Processing Systems Conference (NeurIPS)</a> is a major inter-disciplinary event that brings together researchers and practicioners in machine learning, computer vision, natural language processing, optimization, statistics, but also neuroscience, natural sciences, social sciences, etc. This year, at the thirty-seventh edition of NeurIPS, the <a href="../">valeo.ai</a> team will present 4 papers in the main conference and 1 in the workshops.</p> <p>Notably, we explore perception via different sensors, e.g., audio, on the path towards increasingly autonomous systems. We also study the interaction between different sensing modalities (images, language, Lidar point clouds) and advance a tri-modal self-supervised learning algorithm for 3D semantic voxel occupancy prediction from a rig of cameras mounted on a vehicle. We further show how to obtain robust deep models starting from pre-trained foundation models finetuned with reinforcement learning from human feedback. Finally, we analyze different generative models (diffusion models, GANs) and advance a unification framework considering them as instances of Particle Models.</p> <p>We will be happy to discuss more about these projects and ideas, and share our exciting ongoing research. Take a quick view of our papers below and come meet us at the posters or catch us for a coffee in the hallways.</p> <h2 id="resilient-multiple-choice-learning-a-learned-scoring-scheme-with-application-to-audio-scene-analysis">Resilient Multiple Choice Learning: A learned scoring scheme with application to audio scene analysis</h2> <h4 id="authors-victor-letzelter-mathieu-fontaine-mickaël-chen-patrick-pérez-slim-essid-gaël-richard">Authors: Victor Letzelter, Mathieu Fontaine, Mickaël Chen, Patrick Pérez, Slim Essid, Gaël Richard</h4> <h4 align="center"> [<a href="https://arxiv.org/abs/2311.01052" target="_blank" rel="noopener noreferrer">Paper</a>]    [<a href="https://github.com/Victorletzelter/code-rMCL" target="_blank" rel="noopener noreferrer">Code</a>]    [<a href="../publications/rmcl/">Project page</a>]</h4> <p>In this work, we tackle ambiguous machine learning tasks, where single predictions don’t suffice due to the task’s nature or inherent uncertainties. We introduce a robust multi-hypotheses framework that is capable of deterministically offering a range of plausible predictions at inference time. Our experiments on both synthetic data and real-world audio data affirm the potential and versatility of our method. Check out the paper and the code for more details.</p> <p><img src="/assets/img/posts/2023_neurips/training_dynamics.gif" alt="rmcl_overview" height="100%" width="100%"></p> <p>This problem involves estimating a conditional distribution that is dependent on the input. The accompanying animation illustrates the early stages in the evolution of our model’s learning process, highlighting how it progressively refines its predictions (represented by shaded blue points) to the actual data distribution (indicated by green points), which varies with the input ‘t’.</p> <hr> <h2 id="pop-3d-open-vocabulary-3d-occupancy-prediction-from-images">POP-3D: Open-Vocabulary 3D Occupancy Prediction from Images</h2> <h4 id="authors-antonin-vobecky-oriane-siméoni-david-hurych-spyros-gidaris-andrei-bursuc-patrick-pérez-josef-sivic">Authors: Antonin Vobecky, Oriane Siméoni, David Hurych, Spyros Gidaris, Andrei Bursuc, Patrick Pérez, Josef Sivic</h4> <h4 align="center"> [<a href="https://openreview.net/forum?id=eBXM62SqKY" target="_blank" rel="noopener noreferrer">Paper</a>]    [<a href="https://github.com/vobecant/POP3D" target="_blank" rel="noopener noreferrer">Code</a>]    [<a href="https://vobecant.github.io/POP3D" target="_blank" rel="noopener noreferrer">Project page</a>]</h4> <p>POP-3D is an approach to predict open-vocabulary 3D semantic voxel occupancy map from input 2D images to enable 3D grounding, segmentation, and retrieval of free-form language queries.</p> <p><img src="/assets/img/posts/2023_neurips/pop3d-overview.png" alt="pop3d_overview" height="100%" width="100%"></p> <div class="caption">Given surround-view images on the input, our POP-3D outputs voxel occupancy with 3D-language features, which one can query using text, e.g., to obtain zero-shot semantic segmentation. </div> <p>We design a new model architecture for open-vocabulary 3D semantic occupancy prediction. The architecture consists of a 2D-3D encoder together with occupancy prediction and 3D-language heads. The output is a dense voxel map of 3D grounded language embeddings enabling a range of open-vocabulary tasks. Next, we develop a tri-modal self-supervised learning algorithm that leverages three modalities: (i) images, (ii) language, and (iii) LiDAR point clouds and enables training the proposed architecture using a strong pre-trained vision-language model without the need for any 3D manual language annotations.</p> <p><img src="/assets/img/posts/2023_neurips/pop3d-model.png" alt="pop3d_model" height="100%" width="100%"></p> <div class="caption">Overview of POP-3D architecture and training approach.</div> <p>Finally, we demonstrate the strengths of the proposed model quantitatively on several open-vocabulary tasks: Zero-shot 3D semantic segmentation using existing datasets; 3D grounding, and retrieval of free-form language queries, using a small dataset that we propose as an extension of nuScenes.</p> <p><img src="/assets/img/posts/2023_neurips/pop3d-qualitative.png" alt="pop3d_example" height="100%" width="100%"></p> <hr> <h2 id="rewarded-soups-towards-pareto-optimal-alignment-by-interpolating-weights-fine-tuned-on-diverse-rewards">Rewarded soups: towards Pareto-optimal alignment by interpolating weights fine-tuned on diverse rewards</h2> <h4 id="authors-alexandre-ramé-guillaume-couairon-mustafa-shukor-corentin-dancette-jean-baptiste-gaya-laure-soulier-matthieu-cord">Authors: Alexandre Ramé, Guillaume Couairon, Mustafa Shukor, Corentin Dancette, Jean-Baptiste Gaya, Laure Soulier, Matthieu Cord</h4> <h4 align="center"> [<a href="https://arxiv.org/abs/2306.04488" target="_blank" rel="noopener noreferrer">Paper</a>]    [<a href="https://github.com/alexrame/rewardedsoups" target="_blank" rel="noopener noreferrer">Code</a>]    [<a href="https://huggingface.co/spaces/alexrame/rewardedsoups" target="_blank" rel="noopener noreferrer">Project page</a>]</h4> <p>Foundation models are first pre-trained on vast unsupervised datasets and then fine-tuned on labeled data. Reinforcement learning, notably from human feedback (RLHF), can further align the network with the intended usage. Yet the imperfections in the proxy reward may hinder the training and lead to suboptimal results; the diversity of objectives in real-world tasks and human opinions exacerbate the issue. This paper proposes embracing the heterogeneity of diverse rewards by following a multi-policy strategy. Rather than focusing on a single a priori reward, we aim for Pareto-optimal generalization across the entire space of preferences. To this end, we propose rewarded soup, first specializing multiple networks independently (one for each proxy reward) and then interpolating their weights linearly. This succeeds empirically because we show that the weights remain linearly connected when fine-tuned on diverse rewards from a shared pre-trained initialization. We demonstrate the effectiveness of our approach for text-to-text (summarization, Q&amp;A, helpful assistant, review), text-image (image captioning, text-to-image generation, visual grounding, VQA), and control (locomotion) tasks. We hope to enhance the alignment of deep models, and how they interact with the world in all its diversity.</p> <p><img src="/assets/img/posts/2023_neurips/rewarded-soups.png" alt="rs_overview" height="100%" width="100%"></p> <div class="caption"> <b>Illustration of the different steps of our proposed rewarded soup (RS).</b> After unsupervised pre-training and supervised fine-tuning, we launch $N$ independent RL fine-tunings on the proxy rewards $\{R_i\}^{N}_{i=1}$. Then we combine the trained networks by interpolation in the weight space. The final weights are adapted at test time by selecting the coefficient $\lambda$.</div> <hr> <h2 id="unifying-gans-and-score-based-diffusion-as-generative-particle-models">Unifying GANs and Score-Based Diffusion as Generative Particle Models</h2> <h4 id="authors-jean-yves-franceschi-mike-gartrell-ludovic-dos-santos-thibaut-issenhuth-emmanuel-de-bézenac-mickaël-chen-alain-rakotomamonjy">Authors: Jean-Yves Franceschi, Mike Gartrell, Ludovic Dos Santos, Thibaut Issenhuth, Emmanuel de Bézenac, Mickaël Chen, Alain Rakotomamonjy</h4> <h4 align="center"> [<a href="https://arxiv.org/abs/2305.16150" target="_blank" rel="noopener noreferrer">Paper</a>]    [Code (coming soon)]</h4> <p>By describing the trajectories of GAN outputs during training with particle evolution equations, we propose an unifying framework for GAN and Diffusion Models. We provide a new insights on the role of the generator network, and as proof of concept validating our theories, we propose methods to train a generator with score-based gradient instead of a discriminator, or to use a discriminator’s gradient flow to generate instead of training a generator.</p> <p><img src="/assets/img/posts/2023_neurips/unify-gan.png" alt="unigan_overview" height="70%" width="70%"></p> <hr> <h2 id="evaluating-the-structure-of-cognitive-tasks-with-transfer-learning">Evaluating the structure of cognitive tasks with transfer learning</h2> <p class="page-description"><a href="https://ai4sciencecommunity.github.io/neurips23.html" target="_blank" rel="noopener noreferrer">NeurIPS Workshop on AI for Scientific Discovery: From Theory to Practice</a></p> <h4 id="authors-bruno-aristimunha-raphael-y-de-camargo-walter-h-lopez-pinaya-sylvain-chevallier-alexandre-gramfort-cedric-rommel">Authors: Bruno Aristimunha, Raphael Y. de Camargo, Walter H. Lopez Pinaya, Sylvain Chevallier, Alexandre Gramfort, Cedric Rommel</h4> <h4 align="center"> [<a href="https://cedricrommel.github.io/assets/pdfs/NeurIPS_2023_AI_for_Science_Workshop.pdf" target="_blank" rel="noopener noreferrer">Paper</a>]    [Code (coming soon)]</h4> <p>Electroencephalography (EEG) decoding is a challenging task due to the limited availability of labeled data. While transfer learning is a promising technique to address this challenge, it assumes that transferable data domains and tasks are known, which is not the case in this setting. This work investigates the transferability of deep learning representations between different EEG decoding tasks.</p> <p><img src="/assets/img/posts/2023_neurips/eval-cog-tasks.png" alt="cog_overview" height="90%" width="90%"></p> <div class="caption"> <b>Learned transferability maps for both datasets.</b> Each node corresponds to a distinct cognitive task. Arrow width represents the average transfer performance when using the representations learned from a source task to decode a target task.</div> <p>We conduct extensive experiments using state-of-the-art decoding models on two recently released EEG datasets, ERPCore and M3CV, containing over 140 subjects and 11 distinct cognitive tasks.</p> <p>From an EEG processing perspective, our results can be used to leverage related datasets for alleviating EEG data scarcity with transfer learning. We show that even with a linear probing transfer method, we are able to boost by up to 28% the performance of some tasks. From a neuroscientific standpoint, our transfer maps provide insights into the hierarchical relations between cognitive tasks, hence enhancing our understanding of how these tasks are connected. We discover for example evidence that certain decoding paradigms elicit specific and narrow brain activities, while others benefit from pre-training on a broad range of representations.</p> </article> </div> </div> <footer class="sticky-bottom mt-5"> <div class="container">valeo.ai research page </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id="></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","");</script> </body> </html>