<!DOCTYPE html> <html lang="en"> <head> <meta name="google-site-verification" content=""/> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>valeo.ai at ICCV 2025 | valeo.ai - valeo.ai research page</title> <meta name="author" content=" "/> <meta name="description" content="Sophia Sirko-Galouchenko, Loïck Chambon, Shashanka Venkataramanan, Tuan-Hung Vu, Andrei Bursuc, Eloi Zablocki"/> <meta name="keywords" content="computer vision, ai, valeo, artificial intelligence, research, deep learning"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light"/> <link rel="shortcut icon" href="/assets/img/valeoai_logo_256x256.png"/> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://valeoai.github.io//posts/iccv-2025"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous"> <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"> </script> <script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"></script> <script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"[%",right:"%]",display:!0},{left:"$",right:"$",display:!1}]})});</script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><img src="/assets/img/valeoai_logo.png" alt="valeo.ai" class="title-logo" height="24px"></a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/"></a> </li> <li class="nav-item "> <a class="nav-link" href="/team/">Team</a> </li> <li class="nav-item "> <a class="nav-link" href="/research/">Research</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">Code &amp; Data</a> </li> <li class="nav-item "> <a class="nav-link" href="/posts/">Posts</a> </li> <li class="nav-item "> <a class="nav-link" href="/interns/">Internships</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">valeo.ai at ICCV 2025</h1> <p class="post-meta">October 8, 2025 • <span class="read-time" title="Estimated read time"> 6 min read </span></p> <p class="post-tags"> <a href="2025"> <i class="fas fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/category/multi-sensor"> <i class="fas fa-tag fa-sm"></i> multi-sensor</a>   <a href="/category/3d-perception"> <i class="fas fa-tag fa-sm"></i> 3d-perception</a>   <a href="/category/foundation"> <i class="fas fa-tag fa-sm"></i> foundation</a>   <a href="/category/limited-supervision"> <i class="fas fa-tag fa-sm"></i> limited-supervision</a>   <a href="/category/zero%20shot"> <i class="fas fa-tag fa-sm"></i> zero shot</a>   <a href="/category/deep-learning"> <i class="fas fa-tag fa-sm"></i> deep-learning</a>   <a href="/category/generalization"> <i class="fas fa-tag fa-sm"></i> generalization</a>   <a href="/category/explainability"> <i class="fas fa-tag fa-sm"></i> explainability</a>   </p> </header> <article class="post-content"> <p>The <a href="https://iccv.thecvf.com/" target="_blank" rel="noopener noreferrer">International Conference on Computer Vision (ICCV)</a> is a leading conference that brings together researchers and practitioners in computer vision and machine learning. At the 2025 edition, the <a href="../">valeo.ai</a> team will present five papers in the main conference. We are also co-organizing the <a href="https://opendrivelab.com/iccv2025/workshop/" target="_blank" rel="noopener noreferrer">Learning to See: Advancing Spatial Understanding for Embodied Intelligence</a> workshop, and contributing to the <a href="https://iccv2025-found-workshop.limitlab.xyz" target="_blank" rel="noopener noreferrer">Foundational Data for Industrial Tech Transfer</a> workshop with a keynote on <a href="https://iccv2025-found-workshop.limitlab.xyz/program" target="_blank" rel="noopener noreferrer"><em>Towards openness of vision foundation models</em></a>.</p> <p>The team will be at ICCV to present these works, exchange ideas, and share our exciting ongoing research. We look forward to seeing you in Honolulu!</p> <p><img src="/assets/img/posts/valeoai_iccv.png" alt="valeo.ai papers at ICCV 2025" height="100%" width="100%"></p> <hr> <p><img src="/assets/img/posts/vai_at_iccv25.jpg" alt="valeo.ai team at ICCV 2025" height="100%" width="100%"></p> <hr> <h3 id="dip-unsupervised-dense-in-context-post-training-of-visual-representations">DIP: Unsupervised Dense In-Context Post-training of Visual Representations</h3> <h5 id="authors-sophia-sirko-galouchenko-antonin-vobecky-andrei-bursuc-nicolas-thome-spyros-gidaris">Authors: <a href="https://scholar.google.com/citations?user=3ac3PQMAAAAJ&amp;hl=fr" target="_blank" rel="noopener noreferrer">Sophia Sirko-Galouchenko</a>, <a href="https://vobecant.github.io/" target="_blank" rel="noopener noreferrer">Antonin Vobecky</a>, <a href="https://abursuc.github.io/" target="_blank" rel="noopener noreferrer">Andrei Bursuc</a>, <a href="https://scholar.google.com/citations?user=3ac3PQMAAAAJ&amp;hl=fr" target="_blank" rel="noopener noreferrer">Nicolas Thome</a>, <a href="https://scholar.google.fr/citations?user=7atfg7EAAAAJ&amp;hl=en" target="_blank" rel="noopener noreferrer">Spyros Gidaris</a> </h5> <h5 align="center"> [<a href="https://arxiv.org/abs/2506.18463" target="_blank" rel="noopener noreferrer">Paper</a>]    [<a href="https://github.com/sirkosophia/DIP" target="_blank" rel="noopener noreferrer">Code</a>] </h5> <p><img src="/assets/img/publications/2025_dip.png" alt="dip_teaser" height="100%" width="100%"></p> <p>We introduce DIP, a novel unsupervised post-training method designed to enhance dense image representations in large-scale pretrained vision encoders for in-context scene understanding. Unlike prior approaches that rely on complex self-distillation architectures, our method trains the vision encoder using pseudo-tasks that explicitly simulate downstream in-context scenarios, inspired by meta-learning principles. To enable post-training on unlabeled data, we propose an automatic mechanism for generating in-context tasks that combines a pretrained diffusion model and the vision encoder itself. DIP is simple, unsupervised, and computationally efficient, requiring less than 9 hours on a single A100 GPU. By learning dense representations through pseudo in-context tasks, it achieves strong performance across a wide variety of downstream real-world in-context scene understanding tasks. It outperforms both the initial vision encoder and prior methods, offering a practical and effective solution for improving dense representations.</p> <hr> <h3 id="gaussrender-learning-3d-occupancy-with-gaussian-rendering">GaussRender: Learning 3D Occupancy with Gaussian Rendering</h3> <h5 id="authors-loïck-chambon-éloi-zablocki-alexandre-boulch-mickaël-chen-matthieu-cord">Authors: <a href="https://loickch.github.io/" target="_blank" rel="noopener noreferrer">Loïck Chambon</a>, <a href="https://eloiz.github.io" target="_blank" rel="noopener noreferrer">Éloi Zablocki</a>, <a href="https://boulch.eu/" target="_blank" rel="noopener noreferrer">Alexandre Boulch</a>, <a href="https://scholar.google.com/citations?user=QnRpMJAAAAAJ" target="_blank" rel="noopener noreferrer">Mickaël Chen</a>, <a href="https://cord.isir.upmc.fr/" target="_blank" rel="noopener noreferrer">Matthieu Cord</a> </h5> <h5 align="center"> [<a href="https://arxiv.org/abs/2502.05040" target="_blank" rel="noopener noreferrer">Paper</a>]    [<a href="https://github.com/valeoai/GaussRender" target="_blank" rel="noopener noreferrer">Code</a>] </h5> <p><img src="/assets/img/publications/2025_gaussrender/teaser.png" alt="gaussrender_teaser" height="100%" width="100%"></p> <p>Understanding the 3D geometry and semantics of driving scenes is critical for safe autonomous driving. Recent advances in 3D occupancy prediction improve scene representation but often suffer from spatial inconsistencies, leading to floating artifacts and poor surface localization. Existing voxel-wise losses (e.g., cross-entropy) fail to enforce geometric coherence. GaussRender is a module that improves 3D occupancy learning by enforcing projective consistency. The key idea is to project both predicted and ground-truth 3D occupancy into 2D camera views for supervision, penalizing inconsistent 3D configurations and enforcing coherent 3D structure. To achieve this efficiently, GaussRender leverages differentiable rendering with Gaussian splatting. It integrates seamlessly with existing architectures, requires no inference-time modifications, and significantly improves geometric fidelity across multiple benchmarks (SurroundOcc-nuScenes, Occ3D-nuScenes, SSCBench-KITTI360) and 3D occupancy models (TPVFormer, SurroundOcc, Symphonies).</p> <hr> <div class="row"> <div class="col-sm mt-3 mt-md-0" style="padding-right: 5px;padding-left: 5px;"> <img src="../../assets/img/publications/2025_gaussrender/demo_scene_0003.gif" class="img-fluid rounded z-depth-1"> <div class="caption"> Scene visualization 1 </div> </div> <div class="col-sm mt-3 mt-md-0" style="padding-right: 5px;padding-left: 5px;"> <img src="../../assets/img/publications/2025_gaussrender/demo_scene_0013.gif" class="img-fluid rounded z-depth-1"> <div class="caption"> Scene visualization 2 </div> </div> </div> <div class="caption"> GaussRender is a 3D Occupancy module that can be plugged into any 3D Occupancy model to enhance its predictions and ensure 2D-3D consistency while improving mIoU, IoU, and RayIoU. </div> <hr> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <img src="../../assets/img/publications/2025_gaussrender/pipeline.png" class="img-fluid rounded z-depth-1"> <div class="caption"> GaussRender can be plugged into any model. The core idea is to transform voxels into Gaussians before performing a depth and semantic rendering. </div> </div> </div> <h4 align="center">Results</h4> <p>GaussRender can be plugged into any 3D model. Dedicated experiments on multiple 3D benchmarks (SurroundOcc-nuScenes, Occ3D-nuScenes, SSCBench-KITTI360) and models (TPVFormer, SurroundOcc, Symphonies) demonstrate its performance.</p> <h5>Occ3D-nuScenes</h5> <table> <thead> <tr> <th>Models</th> <th><a href="https://arxiv.org/abs/2502.05040" target="_blank" rel="noopener noreferrer">TPVFormer (ours)</a></th> <th><a href="https://arxiv.org/abs/2302.07817" target="_blank" rel="noopener noreferrer">TPVFormer</a></th> <th><a href="https://arxiv.org/abs/2502.05040" target="_blank" rel="noopener noreferrer">SurroundOcc (ours)</a></th> <th><a href="https://arxiv.org/abs/2303.09551" target="_blank" rel="noopener noreferrer">SurroundOcc</a></th> <th><a href="https://arxiv.org/abs/2304.05316" target="_blank" rel="noopener noreferrer">OccFormer</a></th> <th><a href="https://arxiv.org/abs/2309.09502" target="_blank" rel="noopener noreferrer">RenderOcc</a></th> </tr> <tr> <th>Type</th> <th>w/ GaussRender</th> <th>base</th> <th>w/ GaussRender</th> <th>base</th> <th>base</th> <th>base</th> </tr> </thead> <tbody> <tr> <td>mIoU</td> <td> <strong>30.48 🥇</strong> <span style="color: green;">(+2.65)</span> </td> <td>27.83</td> <td>30.38 🥈 <span style="color: green;">(+1.17)</span> </td> <td>29.21</td> <td>21.93</td> <td>26.11</td> </tr> <tr> <td>RayIoU</td> <td> <strong>38.3 🥇</strong> <span style="color: green;">(+1.1)</span> </td> <td>37.2</td> <td>37.5 🥈 <span style="color: green;">(+2.0)</span> </td> <td>35.5</td> <td>-</td> <td>19.5</td> </tr> </tbody> </table> <h5>SurroundOcc-nuScenes</h5> <table> <thead> <tr> <th>Models</th> <th><a href="https://arxiv.org/abs/2502.05040" target="_blank" rel="noopener noreferrer">TPVFormer (ours)</a></th> <th><a href="https://arxiv.org/abs/2302.07817" target="_blank" rel="noopener noreferrer">TPVFormer</a></th> <th><a href="https://arxiv.org/abs/2502.05040" target="_blank" rel="noopener noreferrer">SurroundOcc (ours)</a></th> <th><a href="https://arxiv.org/abs/2303.09551" target="_blank" rel="noopener noreferrer">SurroundOcc</a></th> <th><a href="https://arxiv.org/abs/2304.05316" target="_blank" rel="noopener noreferrer">OccFormer</a></th> <th><a href="https://arxiv.org/abs/2412.04384" target="_blank" rel="noopener noreferrer">GaussianFormerv2</a></th> </tr> <tr> <th>Type</th> <th>w/ GaussRender</th> <th>base</th> <th>w/ GaussRender</th> <th>base</th> <th>base</th> <th>base</th> </tr> </thead> <tbody> <tr> <td>IoU</td> <td>32.05 🥈 <span style="color: green;">(+1.19)</span> </td> <td>30.86</td> <td> <strong>32.61 🥇</strong> <span style="color: green;">(+1.12)</span> </td> <td>31.49</td> <td>31.39</td> <td>30.56</td> </tr> <tr> <td>mIoU</td> <td>20.58 🥈 <span style="color: green;">(+3.48)</span> </td> <td>17.10</td> <td> <strong>20.82 🥇</strong> <span style="color: green;">(+0.52)</span> </td> <td>20.30</td> <td>19.03</td> <td>20.02</td> </tr> </tbody> </table> <h5>SSCBench-KITTI360</h5> <table> <thead> <tr> <th>Models</th> <th><a href="https://arxiv.org/abs/2502.05040" target="_blank" rel="noopener noreferrer">SurroundOcc (ours)</a></th> <th><a href="https://arxiv.org/abs/2303.09551" target="_blank" rel="noopener noreferrer">SurroundOcc</a></th> <th><a href="https://arxiv.org/abs/2502.05040" target="_blank" rel="noopener noreferrer">Symphonies (ours)</a></th> <th><a href="https://arxiv.org/abs/2306.15670" target="_blank" rel="noopener noreferrer">Symphonies</a></th> <th><a href="https://arxiv.org/abs/2304.05316" target="_blank" rel="noopener noreferrer">OccFormer</a></th> <th><a href="https://arxiv.org/abs/2112.00726" target="_blank" rel="noopener noreferrer">MonoScene</a></th> </tr> <tr> <th>Type</th> <th>w/ GaussRender</th> <th>base</th> <th>w/ GaussRender</th> <th>base</th> <th>base</th> <th>base</th> </tr> </thead> <tbody> <tr> <td>IoU</td> <td>38.62 <span style="color: green;">(+0.11)</span> </td> <td>38.51</td> <td> <strong>44.08 🥇</strong> <span style="color: green;">(+0.68)</span> </td> <td>43.40 🥈</td> <td>40.27</td> <td>37.87</td> </tr> <tr> <td>mIoU</td> <td>13.34 <span style="color: green;">(+0.26)</span> </td> <td>13.08</td> <td> <strong>18.11 🥇</strong> <span style="color: green;">(+0.29)</span> </td> <td>17.82 🥈</td> <td>13.81</td> <td>12.31</td> </tr> </tbody> </table> <hr> <h3 id="mosic-optimal-transport-motion-trajectories-for-dense-self-supervised-learning">MoSiC: Optimal-Transport Motion Trajectories for Dense Self-Supervised Learning</h3> <h5 id="authors-mohammadreza-salehi-shashanka-venkataramanan-ioana-simion-efstratios-gavves-cees-snoek-yuki-asano">Authors: <a href="https://scholar.google.com/citations?user=kpT3gcsAAAAJ&amp;hl=en" target="_blank" rel="noopener noreferrer">Mohammadreza Salehi</a>, <a href="https://shashankvkt.github.io/" target="_blank" rel="noopener noreferrer">Shashanka Venkataramanan</a>, Ioana Simion, <a href="https://www.egavves.com/" target="_blank" rel="noopener noreferrer">Efstratios Gavves</a>, <a href="https://www.ceessnoek.info/" target="_blank" rel="noopener noreferrer">Cees Snoek</a>, <a href="https://yukimasano.github.io/" target="_blank" rel="noopener noreferrer">Yuki Asano</a> </h5> <h5 align="center"> [<a href="https://arxiv.org/abs/2506.08694" target="_blank" rel="noopener noreferrer">Paper</a>]    [<a href="https://github.com/SMSD75/MoSiC/tree/main" target="_blank" rel="noopener noreferrer">Code</a>] </h5> <p><img src="/assets/img/publications/2025_mosic.png" alt="mosic_teaser" height="100%" width="100%"></p> <p>Dense self-supervised learning has shown great promise for learning pixel- and patch-level representations, but extending it to videos remains challenging due to complex motion dynamics. Existing approaches struggle under object deformations, occlusions, and camera movement, leading to inconsistent feature learning over time. In this work, we introduce MoSiC, a motion-guided self-supervised framework that clusters dense point tracks to learn spatiotemporally consistent representations. Using an off-the-shelf point tracker, we extract long-range motion trajectories and optimize feature clustering with a momentum-encoder-based optimal transport mechanism. Temporal coherence is enforced by propagating cluster assignments along tracked points, ensuring feature consistency across views despite viewpoint changes. By leveraging motion as an implicit supervisory signal and initializing from strong image-pretrained models, MoSiC learns robust representations that generalize across frames. Our approach improves state-of-the-art performance by 1% to 6% across six image and video datasets and four evaluation benchmarks.</p> <hr> <h3 id="floss-free-lunch-in-open-vocabulary-semantic-segmentation">FLOSS: Free Lunch in Open-vocabulary Semantic Segmentation</h3> <h5 id="authors-yasser-benigmim-mohammad-fahes-tuan-hung-vu-andrei-bursuc-raoul-de-charette">Authors: <a href="https://yasserben.github.io/" target="_blank" rel="noopener noreferrer">Yasser Benigmim</a>, <a href="https://mfahes.github.io/" target="_blank" rel="noopener noreferrer">Mohammad Fahes</a>, <a href="https://tuanhungvu.github.io/" target="_blank" rel="noopener noreferrer">Tuan-Hung Vu</a>, <a href="https://abursuc.github.io/" target="_blank" rel="noopener noreferrer">Andrei Bursuc</a>, <a href="https://team.inria.fr/rits/membres/raoul-de-charette/" target="_blank" rel="noopener noreferrer">Raoul de Charette</a> </h5> <h5 align="center"> [<a href="https://arxiv.org/abs/2504.10487" target="_blank" rel="noopener noreferrer">Paper</a>]    [<a href="https://github.com/yasserben/FLOSS" target="_blank" rel="noopener noreferrer">Code</a>] </h5> <p><img src="/assets/img/publications/2025_floss.jpg" alt="floss_teaser" height="100%" width="100%"></p> <p>In Open-Vocabulary Semantic Segmentation (OVSS), class-wise text embeddings are usually averaged over multiple templates (e.g., “a photo of <class>", "a sketch of <class>") to form classifiers. We show that for each class, there exist single-template classifiers—termed class-experts—that outperform conventional averaged classifiers. To identify these class-experts without labeled data, we leverage class-wise prediction entropy and select the lowest-entropy classifiers as the most reliable. We then fuse the outputs of these class-experts using a novel plug-and-play process called FLOSS. FLOSS is complementary to existing OVSS methods, requiring no additional labels or training, yet consistently improves performance. Extensive experiments demonstrate that FLOSS enhances state-of-the-art OVSS models, generalizes across datasets with distribution shifts, and yields strong improvements in low-data scenarios with only a few unlabeled images.</class></class></p> <hr> <h3 id="analyzing-fine-tuning-representation-shift-for-multimodal-llms-steering-alignment">Analyzing Fine-tuning Representation Shift for Multimodal LLMs Steering Alignment</h3> <h5 id="authors-pegah-khayatan-mustafa-shukor-jayneel-parekh-matthieu-cord">Authors: <a href="https://pegah-kh.github.io/" target="_blank" rel="noopener noreferrer">Pegah Khayatan</a>, <a href="https://mustafashukor.github.io/" target="_blank" rel="noopener noreferrer">Mustafa Shukor</a>, <a href="https://jayneelparekh.github.io/" target="_blank" rel="noopener noreferrer">Jayneel Parekh</a>, <a href="https://cord.isir.upmc.fr/" target="_blank" rel="noopener noreferrer">Matthieu Cord</a> </h5> <h5 align="center"> [<a href="https://arxiv.org/abs/2501.03012" target="_blank" rel="noopener noreferrer">Paper</a>]    [<a href="https://github.com/mshukor/xl-vlms/" target="_blank" rel="noopener noreferrer">Code</a>] </h5> <p><img src="/assets/img/publications/2025_xlxvlm.png" alt="xlvlm_teaser" height="100%" width="100%"></p> <p>Multimodal LLMs have achieved remarkable proficiency in understanding multimodal inputs, yet little attention has been given to explaining how their internal representations evolve during training. Most explainability research focuses only on final model states, ignoring dynamic representational shifts. In this work, we systematically analyze the evolution of hidden state representations during fine-tuning, revealing how models adapt to new multimodal tasks. Using a concept-based approach, we map hidden states to interpretable visual and textual concepts, enabling us to trace concept changes across modalities as training progresses. We introduce shift vectors to capture these changes, allowing recovery of fine-tuned concepts from the original model. Furthermore, we demonstrate practical applications in model steering, such as adjusting answer types, caption styles, or biasing responses without additional training. This work provides novel insights into multimodal representation adaptation and offers tools for interpreting and controlling fine-tuned multimodal LLMs.</p> </article> </div> </div> <footer class="sticky-bottom mt-5"> <div class="container">valeo.ai research page </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id="></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","");</script> </body> </html>