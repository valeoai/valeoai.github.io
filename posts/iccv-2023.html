<!DOCTYPE html> <html lang="en"> <head> <meta name="google-site-verification" content=""/> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>valeo.ai at ICCV 2023 | valeo.ai - valeo.ai research page</title> <meta name="author" content=" "/> <meta name="description" content="Gilles Puy, Tuan-Hung Vu, Oriane Siméoni, Matthieu Cord, Cédric Rommel, Andrei Bursuc"/> <meta name="keywords" content="computer vision, ai, valeo, artificial intelligence, research, deep learning"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light"/> <link rel="shortcut icon" href="/assets/img/valeoai_logo_256x256.png"/> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://valeoai.github.io//posts/iccv-2023"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous"> <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"> </script> <script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"></script> <script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"[%",right:"%]",display:!0},{left:"$",right:"$",display:!1}]})});</script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><img src="/assets/img/valeoai_logo.png" alt="valeo.ai" class="title-logo" height="24px"></a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/"></a> </li> <li class="nav-item "> <a class="nav-link" href="/team/">Team</a> </li> <li class="nav-item "> <a class="nav-link" href="/research/">Research</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">Code &amp; Data</a> </li> <li class="nav-item "> <a class="nav-link" href="/posts/">Posts</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">valeo.ai at ICCV 2023</h1> <p class="post-meta">September 26, 2023 • <span class="read-time" title="Estimated read time"> 12 min read </span></p> <p class="post-tags"> <a href="2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a>   ·   <a href="/category/3d%20perception"> <i class="fas fa-tag fa-sm"></i> 3d perception</a>   <a href="/category/multi-sensor"> <i class="fas fa-tag fa-sm"></i> multi-sensor</a>   <a href="/category/limited%20supervision"> <i class="fas fa-tag fa-sm"></i> limited supervision</a>   <a href="/category/reliability"> <i class="fas fa-tag fa-sm"></i> reliability</a>   <a href="/category/domain-adaptation"> <i class="fas fa-tag fa-sm"></i> domain-adaptation</a>   </p> </header> <article class="post-content"> <p>The <a href="https://iccv2023.thecvf.com/" target="_blank" rel="noopener noreferrer">IEEE / CVF International Conference on Computer Vision (ICCV)</a> is a landmark event for the increasingly large and diverse community of researchers in computer vision and machine learning. This year, ICCV takes place in Paris, home of the <a href="../">valeo.ai</a> team. From interns to senior researchers, the valeo.ai team will participate in mass at ICCV and will be looking forward to welcoming you and talking about the exciting progress and ideas in the field.</p> <p>At ICCV 2023 we will present 5 papers in the main conference and 3 in the workshops. We are also organizing 2 tutorials with 2 challenges (<a href="https://valeoai.github.io/bravo/">BRAVO</a> and <a href="https://uncv2023.github.io/" target="_blank" rel="noopener noreferrer">UNCV</a>) and a tutorial (<a href="https://abursuc.github.io/many-faces-reliability/" target="_blank" rel="noopener noreferrer">Many Faces of Reliability</a>). Take a quick view of our papers in the conference and come meet us at the posters, at our booth or in the hallway.</p> <h2 id="using-a-waffle-iron-for-automotive-point-cloud-semantic-segmentation">Using a Waffle Iron for Automotive Point Cloud Semantic Segmentation</h2> <h4 id="authors-gilles-puy-alexandre-boulch-renaud-marlet">Authors: Gilles Puy, Alexandre Boulch, Renaud Marlet</h4> <h4 align="center"> [<a href="https://arxiv.org/abs/2301.10100" target="_blank" rel="noopener noreferrer">Paper</a>]    [<a href="https://github.com/valeoai/WaffleIron" target="_blank" rel="noopener noreferrer">Code</a>]    [<a href="../publications/waffleiron/">Project page</a>]</h4> <p>Semantic segmentation of point clouds delivered by lidars permits autonomous vehicles to make sense of their 3D surrounding environment. Sparse convolutions have become a de-facto tool to process these large outdoor point clouds. The top performing methods on public benchmarks, such SemanticKITTI or nuScenes, all leverage sparse convolutions. Nevertheless, despite their undeniable success and efficiency, these convolutions remain available in a limited number of deep learning frameworks and hardware platforms. In this work, we propose an alternative backbone built with tools broadly available (such as 2D and 1D convolutions) but that still reaches the level of performance of the top methods on automotive datasets.</p> <p>We propose a point-based backbone, called WaffleIron, which is essentially built using standard MLPs and dense 2D convolutions, both readily available in all deep learning frameworks thanks to their wide use in the field of computer vision. The architecture of this backbone is illustrated in the figure below. It is inspired by the recent MLP-Mixer. It takes as input a point cloud with a token associated to each point. All these point tokens are then updated by a sequence of layers, each containing a token-mixing step (made of dense 2D convolutions) and a channel-mixing step (made of a MLP shared across points).</p> <p><img src="/assets/img/posts/2023_iccv/waffleiron.png" alt="waffle_overview" height="70%" width="70%"></p> <div class="caption">The WaffleIron backbone takes as input point tokens, provided by an embedding layer (not represented), and updates these point representations L times via a point token-mixing layer (containing the WI block) followed by a channel-mixing layer. The WI block consists of a 2D projection along one of the main axes, a feed-forward network (FFN) with two dense channel-wise 2D convolutions with a ReLU activation in the hidden layer, and a simple copy of the 2D features to the 3D points. The channel-mixing layer contains a batch-norm, a MLP shared across each point, and a residual connection. The WaffleIron backbone is free of any point downsampling or upsampling layer, farthest point sampling, nearest neighbor search, or sparse convolution. </div> <p>WaffleIron has three main hyperparameters to tune: the depth L, the width F and the resolution of the 2D grid. We show that these parameters are easy to tune: the performance increases with the network width F and depth L, until an eventual saturation; we observe stable results over a wide range of values for the resolution of the 2D grid.</p> <p>In our paper, we also provide many details on how to train WaffleIron to reach the performance of top-entries on two autonomous driving benchmarks: SemanticKITTI and nuScenes.</p> <hr> <h2 id="pøda-prompt-driven-zero-shot-domain-adaptation">PØDA: Prompt-driven Zero-shot Domain Adaptation</h2> <h4 id="authors-mohammad-fahes-tuan-hung-vu-andrei-bursuc-patrick-pérez-raoul-de-charette">Authors: Mohammad Fahes, Tuan-Hung Vu, Andrei Bursuc, Patrick Pérez, Raoul de Charette</h4> <h4 align="center"> [<a href="https://arxiv.org/abs/2212.03241" target="_blank" rel="noopener noreferrer">Paper</a>]    [<a href="https://github.com/astra-vision/PODA" target="_blank" rel="noopener noreferrer">Code</a>]    [<a href="https://www.youtube.com/watch?v=kataxQoPuSE" target="_blank" rel="noopener noreferrer">Video</a>]    [<a href="https://astra-vision.github.io/PODA/" target="_blank" rel="noopener noreferrer">Project page</a>]</h4> <p>Domain adaptation has been vastly investigated in computer vision but still requires access to target images at train time, which might be intractable in some uncommon conditions. In this paper, we propose the task of ‘Prompt-driven Zero-shot Domain Adaptation’, where we adapt a model trained on a source domain using only a general description in natural language of the target domain, i.e., a prompt. First, we leverage a pre-trained contrastive vision-language model (CLIP) to optimize affine transformations of source features, steering them towards the target text embedding while preserving their content and semantics. To achieve this, we propose Prompt-driven Instance Normalization (PIN). Second, we show that these prompt-driven augmentations can be used to perform zero-shot domain adaptation for semantic segmentation. Experiments demonstrate that our method significantly outperforms CLIP-based style transfer baselines on several datasets for the downstream task at hand, even surpassing one-shot unsupervised domain adaptation. A similar boost is observed on object detection and image classification</p> <p><img src="/assets/img/posts/2023_iccv/poda.png" alt="poda_overview" height="80%" width="80%"></p> <div class="caption">We perform zero-shot adaptation with natural language prompts. PØDA enables the adaptation of a segmenter model (here, DeepLabv3+ trained on the source dataset Cityscapes) to unseen conditions with only a prompt. Source-only predictions are shown as smaller segmentation masks to the left or right of the test images. </div> <hr> <h2 id="you-never-get-a-second-chance-to-make-a-good-first-impression-seeding-active-learning-for-3d-semantic-segmentation">You Never Get a Second Chance To Make a Good First Impression: Seeding Active Learning for 3D Semantic Segmentation</h2> <h4 id="authors-nermin-samet-oriane-siméoni-gilles-puy-georgy-ponimatkin-renaud-marlet-vincent-lepetit">Authors: Nermin Samet, Oriane Siméoni, Gilles Puy, Georgy Ponimatkin, Renaud Marlet, Vincent Lepetit</h4> <h4 align="center"> [<a href="https://arxiv.org/abs/2304.11762" target="_blank" rel="noopener noreferrer">Paper</a>]    [<a href="https://github.com/nerminsamet/seedal" target="_blank" rel="noopener noreferrer">Code</a>]</h4> <p>We are interested in the efficient annotation of sparse 3D point clouds (as captured indoors by depth cameras or outdoors by automotive lidars) for semantic segmentation. Active Learning (AL) iteratively selects relevant data fractions to annotate within a given budget, but requires a first fraction of the dataset (a ’seed’) to be already annotated to estimate the benefit of annotating other data fractions. We show that the choice of the seed can significantly affect the performance of many AL methods and propose a method, named SeedAL, for automatically constructing a seed that will ensure good performance for AL. Assuming that images of the point clouds are available, which is common, our method relies on powerful unsupervised image features to measure the diversity of the point clouds. It selects the point clouds for the seed by optimizing the diversity under an annotation budget, which can be done by solving a linear optimization problem. Our experiments demonstrate the effectiveness of our approach compared to random seeding and existing methods on both the S3DIS and SemanticKitti datasets.</p> <p><img src="/assets/img/posts/2023_iccv/seedal.png" alt="seedal_overview" height="70%" width="70%"></p> <div class="caption"> <b>Impact of active learning seed on performance. </b>We show the variability of results obtained with 20 different random seeds (blue dashed lines), within an initial annotation budget of 3% of the dataset, when using various active learning methods for 3D semantic segmentation of S3DIS. We compare it to the result obtained with our seed selection strategy (solid red line), named SeedAL, which performs better or on par with the best (lucky) random seeds among 20, and “protects” from very bad (unlucky) random seeds.</div> <hr> <h2 id="ep-alm-efficient-perceptual-augmentation-of-language-models">eP-ALM: Efficient Perceptual Augmentation of Language Models</h2> <h4 id="authors-mustafa-shukor-corentin-dancette-matthieu-cord">Authors: Mustafa Shukor, Corentin Dancette, Matthieu Cord</h4> <h4 align="center"> [<a href="https://arxiv.org/abs/2303.11403" target="_blank" rel="noopener noreferrer">Paper</a>]    [<a href="https://github.com/mshukor/eP-ALM" target="_blank" rel="noopener noreferrer">Code</a>]    [<a href="https://mshukor.github.io/eP-ALM.github.io/" target="_blank" rel="noopener noreferrer">Project page</a>]</h4> <p>eP-ALM aims to augment large language models (LLMs) with perception. While most existing approaches train a large number of parameters and rely on extensive multimodal pre-training, we investigate the minimal computational effort required to adapt unimodal models to multimodal tasks. We show that by freezing more than 99% of total parameters, training only one linear projection layer and prepending only one trainable token, our approach (dubbed eP-ALM) significantly outperforms other baselines on VQA and captioning for image, video and audio modalities.</p> <p><img src="/assets/img/posts/2023_iccv/ep-alm.png" alt="epalm_overview" height="70%" width="70%"></p> <div class="caption"> <b>Illustration of the adaptation mechanism in eP-ALM.</b> The perceptual input (image/video/audio) is fed to the perceptual encoder E (e.g., ViT) and the corresponding text to the LM (e.g., OPT), which then generates a text conditioned on the perceptual input. The multimodal interaction is done via the [CLS] tokens acting as Perceptual Prompt, and are extracted from the last layers of the encoder, then injected in the last layers of LM, after passing by the Linear Connection C. The previous [CLS] token is replaced by the new one coming from a deeper layer, keeping the number of tokens fixed. The first layers (grayed) of each model are kept intact without any modality interaction. We ease the adaptation with a Soft Prompt that is prepended to the input of LM. </div> <hr> <h2 id="zero-shot-spatial-layout-conditioning-for-text-to-image-diffusion-models">Zero-shot spatial layout conditioning for text-to-image diffusion models</h2> <h4 id="authors-guillaume-couairon-marlène-careil-matthieu-cord-stéphane-lathuilière-jakob-verbeek">Authors: Guillaume Couairon, Marlène Careil, Matthieu Cord, Stéphane Lathuilière, Jakob Verbeek</h4> <h4 align="center"> [<a href="https://arxiv.org/abs/2306.13754" target="_blank" rel="noopener noreferrer">Paper</a>]</h4> <p>Large-scale text-to-image diffusion models have considerably improved the state of the art in generative image modeling, and provide an intuitive and powerful user interface to drive the image generation process. In this paper, we propose ZestGuide, a “zero-shot” segmentation guidance approach that can be integrated into pre-trained text-image diffusion models, and requires no additional training. It exploits the implicit segmentation maps that can be extracted from cross-attention layers, and uses them to align generation with input masks.</p> <p><img src="/assets/img/posts/2023_iccv/zest-guide.png" alt="zest_overview" height="70%" width="70%"></p> <div class="caption">ZestGuide generates images conditioned on segmentation maps with corresponding free-form textual descriptions. </div> <hr> <h2 id="diffhpe-robust-coherent-3d-human-pose-lifting-with-diffusion">DiffHPE: Robust, Coherent 3D Human Pose Lifting with Diffusion</h2> <p class="page-description"><a href="https://web.northeastern.edu/smilelab/amfg2023/" target="_blank" rel="noopener noreferrer">ICCV Workshop on Analysis and Modeling of Faces and Gestures</a></p> <h4 id="authors-cédric-rommel-eduardo-valle-mickaël-chen-souhaiel-khalfaoui-renaud-marlet-matthieu-cord-patrick-pérez">Authors: Cédric Rommel, Eduardo Valle, Mickaël Chen, Souhaiel Khalfaoui, Renaud Marlet, Matthieu Cord, Patrick Pérez</h4> <h4 align="center"> [<a href="https://arxiv.org/abs/2309.01575" target="_blank" rel="noopener noreferrer">Paper</a>]    [<a href="https://github.com/valeoai/diffhpe" target="_blank" rel="noopener noreferrer">Code</a>]    [<a href="../publications/diffhpe">Project page</a>]</h4> <p>Diffusion models are making waves across various domains, including computer vision, natural language processing and time-series analysis. However, its application to purely predictive tasks, such as 3D human pose estimation (3D-HPE), remains largely unexplored. While a few pioneering works have shown promising performance metrics in 3D-HPE, the understanding of the benefits of diffusion models over classical supervision — as well as key design choices — is still in its infancy. In this work, we address those concerns, providing an in-depth analysis of the effects of diffusion models on 3D-HPE.</p> <p><img src="/assets/img/posts/2023_iccv/diffhpe.gif" alt="diffhpe_overview" height="100%" width="100%"></p> <div class="caption">Poses across the learned reverse diffusion process converge to an accurate 3D reconstruction of the corresponding 2D pose in pixel space.</div> <p>More precisely, we propose DiffHPE, a novel strategy to use diffusion models in 3D-HPE, and show that combining diffusion with pre-trained supervised models allows to outperform both pure diffusion and pure supervised models trained separately. Our analysis demonstrates not only that the diffusion framework can be used to enhance accuracy, as previously understood, but also that it can improve robustness and coherence. Namely, our experiments showcase how poses estimated with diffusion models’ display better bilateral and temporal coherence, and are more robust to occlusions, even when not perfectly trained for the latter.</p> <hr> <h2 id="challenges-of-using-real-world-sensory-inputs-for-motion-forecasting-in-autonomous-driving">Challenges of Using Real-World Sensory Inputs for Motion Forecasting in Autonomous Driving</h2> <p class="page-description"><a href="https://sites.google.com/view/road-plus-plus" target="_blank" rel="noopener noreferrer">ROAD++: The Second Workshop and Challenge on Event Detection for Situation Awareness in Autonomous Driving</a></p> <h4 id="authors-yihong-xu-loïck-chambon-éloi-zablocki-mickaël-chen-matthieu-cord-patrick-pérez">Authors: Yihong Xu, Loïck Chambon, Éloi Zablocki, Mickaël Chen, Matthieu Cord, Patrick Pérez</h4> <h4 align="center"> [<a href="https://arxiv.org/abs/2306.09281" target="_blank" rel="noopener noreferrer">Paper</a>]    [<a href="../publications/real-world-forecasting/">Project page</a>]</h4> <p>Motion forecasting is crucial in enabling autonomous vehicles to anticipate the future trajectories of surrounding agents. To do so, it requires solving mapping, detection, tracking, and then forecasting problems, in a multi-step pipeline. In this complex system, advances in conventional forecasting methods have been made using curated data, i.e., with the assumption of perfect maps, detection, and tracking. This paradigm, however, ignores any errors from upstream modules. Meanwhile, an emerging end-to-end paradigm, that tightly integrates the perception and forecasting architectures into joint training, promises to solve this issue. So far, however, the evaluation protocols between the two methods were incompatible and their comparison was not possible. In fact, and perhaps surprisingly, conventional forecasting methods are usually not trained nor tested in real-world pipelines (e.g., with upstream detection, tracking, and mapping modules). In this work, we aim to bring forecasting models closer to real-world deployment. First, we propose a unified evaluation pipeline for forecasting methods with real-world perception inputs, allowing us to compare the performance of conventional and end-to-end methods for the first time. Second, our in-depth study uncovers a substantial performance gap when transitioning from curated to perception-based data. In particular, we show that this gap (1) stems not only from differences in precision but also from the nature of imperfect inputs provided by perception modules, and that (2) is not trivially reduced by simply finetuning on perception outputs. Based on extensive experiments, we provide recommendations for critical areas that require improvement and guidance towards more robust motion forecasting in the real world. We will release an evaluation library to benchmark models under standardized and practical conditions.</p> <p><img src="/assets/img/posts/2023_iccv/e2e_forecasting.png" alt="forecast_overview" height="90%" width="90%"></p> <div class="caption"> <b>Study overview.</b> We study the challenges of deploying motion forecasting models into the real world when only predicted perception inputs are available. We compare: (1) (top) "conventional methods" (i.e., methods trained on curated input data) where (middle) we directly replace the curated inputs with real-world data, and (2) (bottom) "end-to-end methods" that are trained and used with perception modules. In the real-world setting, evaluation is challenging as the past tracks are estimated with arbitrary identities, making it difficult to establish a direct correspondence to GT identities. Therefore, we propose a matching process (purple) to assign predictions to GT and thus evaluate forecasting performances. Moreover, we study in depth the impact changing from curated data (green) to real-world (orange) mapping, or detection and tracking errors to motion forecasting. </div> <hr> <h2 id="pop-3d-open-vocabulary-3d-occupancy-prediction-from-images">POP-3D: Open-Vocabulary 3D Occupancy Prediction from Images</h2> <p class="page-description"><a href="https://opensun3d.github.io/" target="_blank" rel="noopener noreferrer">ICCV 2023 Workshop on Open-Vocabulary 3D Scene Understanding (OpenSUN 3D)</a></p> <h4 id="authors-antonin-vobecky-oriane-siméoni-david-hurych-spyros-gidaris-andrei-bursuc-patrick-pérez-josef-sivic">Authors: Antonin Vobecky, Oriane Siméoni, David Hurych, Spyros Gidaris, Andrei Bursuc, Patrick Pérez, Josef Sivic</h4> <h4 align="center"> [<a href="https://data.ciirc.cvut.cz/public/projects/2023POP3D/resources/pop3d_paper.pdf" target="_blank" rel="noopener noreferrer">Paper</a>]</h4> <p>We propose an approach to predict a 3D semantic voxel occupancy map from input 2D images with features allowing 3D grounding, segmentation and retrieval of free-form language queries. To this end: We design a new architecture that consists of a 2D-3D encoder together with occupancy prediction and 3D-language heads; We develop a tri-modal self-supervised training that leverages three modalities – images, language and LiDAR point clouds– and enables learning the proposed architecture using a strong pre-trained vision-language model without the need for any 3D manual annotations. We quantitatively evaluate the proposed model on the task of zero-shot 3D semantic segmentation using existing datasets and show results on the tasks of 3D grounding and retrieval of free-form language queries.</p> <p><img src="/assets/img/posts/2023_iccv/pop3d.png" alt="forecast_overview" height="100%" width="100%"></p> <div class="caption"> <b>Method overview.</b>Given surround-view images, POP-3D produces a voxel grid of text-aligned features that support open-vocabulary downstream tasks such as zero-shot occupancy segmentation or text-based grounding and retrieval. </div> </article> </div> </div> <footer class="sticky-bottom mt-5"> <div class="container">valeo.ai research page </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id="></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","");</script> </body> </html>