<!DOCTYPE html> <html lang="en"> <head> <meta name="google-site-verification" content=""/> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>valeo.ai at NeurIPS 2025 | valeo.ai - valeo.ai research page</title> <meta name="author" content=" "/> <meta name="description" content="Loïck Chambon, Spyros Gidaris, Andrei Bursuc, Eloi Zablocki"/> <meta name="keywords" content="computer vision, ai, valeo, artificial intelligence, research, deep learning"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light"/> <link rel="shortcut icon" href="/assets/img/valeoai_logo_256x256.png"/> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://valeoai.github.io//posts/neurips-2025"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous"> <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"> </script> <script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"></script> <script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"[%",right:"%]",display:!0},{left:"$",right:"$",display:!1}]})});</script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><img src="/assets/img/valeoai_logo.png" alt="valeo.ai" class="title-logo" height="24px"></a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/"></a> </li> <li class="nav-item "> <a class="nav-link" href="/team/">Team</a> </li> <li class="nav-item "> <a class="nav-link" href="/research/">Research</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">Code &amp; Data</a> </li> <li class="nav-item "> <a class="nav-link" href="/posts/">Posts</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">valeo.ai at NeurIPS 2025</h1> <p class="post-meta">December 2, 2025 • <span class="read-time" title="Estimated read time"> 6 min read </span></p> <p class="post-tags"> <a href="2025"> <i class="fas fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/category/foundation"> <i class="fas fa-tag fa-sm"></i> foundation</a>   <a href="/category/limited-supervision"> <i class="fas fa-tag fa-sm"></i> limited-supervision</a>   <a href="/category/deep-learning"> <i class="fas fa-tag fa-sm"></i> deep-learning</a>   <a href="/category/generalization"> <i class="fas fa-tag fa-sm"></i> generalization</a>   <a href="/category/explainability"> <i class="fas fa-tag fa-sm"></i> explainability</a>   </p> </header> <article class="post-content"> <p>The <a href="https://neurips.cc/" target="_blank" rel="noopener noreferrer">Neural Information Processing Systems Conference (NeurIPS)</a> is a major inter-disciplinary event that brings together researchers and practicioners in machine learning, computer vision, natural language processing, optimization, statistics, but also neuroscience, natural sciences, social sciences, etc. This year, at the 39th edition of NeurIPS, the <a href="../">valeo.ai</a> team will present 5 papers in the main conference and 1 in the workshops. We are honored to announce that our <a href="https://valeoai.github.io/publications/ipa/">IPA</a> paper on efficient foundation model adaptation has received the outstanding paper award at the <a href="https://sites.google.com/view/ccfm-neurips2025" target="_blank" rel="noopener noreferrer">CCFM workshop</a>. Our team contributed to the technical program committee with multiple reviewers out of whom 1 was awarded top reviewer and 2 as top area chairs.</p> <p>The team will be at NeurIPS to present these works, exchange ideas, and share our exciting ongoing research. We look forward to seeing you in San Diego!</p> <p><img src="/assets/img/posts/2025_neurips_valeoai_papers.png" alt="valeo.ai papers at NeurIPS 2025" height="100%" width="100%"></p> <hr> <h3 id="jafar-jack-up-any-feature-at-any-resolution">JAFAR: Jack up Any Feature at Any Resolution</h3> <h5 id="authors-paul-couairon-loick-chambon--louis-serrano-jean-emmanuel-haugeard-matthieu-cord-nicolas-thome">Authors: <a href="https://scholar.google.fr/citations?user=yQRnP7YAAAAJ&amp;hl=fr" target="_blank" rel="noopener noreferrer">Paul Couairon</a>, <a href="https://loickch.github.io/" target="_blank" rel="noopener noreferrer">Loick Chambon</a> , <a href="https://scholar.google.com/citations?user=fKlo-lUAAAAJ&amp;hl=fr" target="_blank" rel="noopener noreferrer">Louis Serrano</a>, <a>Jean-Emmanuel Haugeard</a>, <a href="https://cord.isir.upmc.fr/" target="_blank" rel="noopener noreferrer">Matthieu Cord</a>, <a href="https://thome.isir.upmc.fr" target="_blank" rel="noopener noreferrer">Nicolas Thome</a> </h5> <h5 align="center"> [<a href="https://arxiv.org/abs/2506.11136" target="_blank" rel="noopener noreferrer">Paper</a>]    [<a href="https://github.com/PaulCouairon/JAFAR" target="_blank" rel="noopener noreferrer">Code</a>] </h5> <p><img src="/assets/img/publications/2025_jafar/teaser.png" alt="jafar_teaser" height="100%" width="100%"></p> <p>Foundation Vision Encoders have become essential for a wide range of dense vision tasks. However, their low-resolution spatial feature outputs necessitate feature upsampling to produce the high-resolution modalities required for downstream tasks. In this work, we introduce JAFAR—a lightweight and flexible feature upsampler that enhances the spatial resolution of visual features from any Foundation Vision Encoder to an arbitrary target resolution. JAFAR employs an attention-based module designed to promote semantic alignment between high-resolution queries—derived from low-level image features—and semantically enriched low-resolution keys, using Spatial Feature Transform (SFT) modulation. Notably, despite the absence of high-resolution supervision, we demonstrate that learning at low upsampling ratios and resolutions generalizes remarkably well to significantly higher output scales. Extensive experiments show that JAFAR effectively recovers fine-grained spatial details and consistently outperforms existing feature upsampling methods across a diverse set of downstream tasks.</p> <hr> <h3 id="dino-foresight-looking-into-the-future-with-dino">DINO-Foresight: Looking into the Future with DINO</h3> <h5 id="authors-efstathios-karypidis-ioannis-kakogeorgiou-spyros-gidaris-nikos-komodakis">Authors: <a href="https://archimedesai.gr/en/researchers/stathis-karypidis" target="_blank" rel="noopener noreferrer">Efstathios Karypidis</a>, <a href="https://scholar.google.com/citations?user=B_dKcz4AAAAJ" target="_blank" rel="noopener noreferrer">Ioannis Kakogeorgiou</a>, <a href="https://gidariss.github.io/" target="_blank" rel="noopener noreferrer">Spyros Gidaris</a>, <a href="https://www.csd.uoc.gr/~komod/" target="_blank" rel="noopener noreferrer">Nikos Komodakis</a> </h5> <h5 align="center"> [<a href="https://arxiv.org/abs/2412.11673" target="_blank" rel="noopener noreferrer">Paper</a>]    [<a href="https://github.com/Sta8is/DINO-Foresight" target="_blank" rel="noopener noreferrer">Code</a>] </h5> <p><img src="/assets/img/publications/2025_dinoforesight/teaser.png" alt="foresight_teaser" height="100%" width="100%"></p> <p>Predicting future dynamics is crucial for applications like autonomous driving and robotics, where understanding the environment is key. Existing pixel-level methods are computationally expensive and often focus on irrelevant details. To address these challenges, we introduce DINO-Foresight, a novel framework that operates in the semantic feature space of pretrained Vision Foundation Models (VFMs). Our approach trains a masked feature transformer in a self-supervised manner to predict the evolution of VFM features over time. By forecasting these features, we can apply off-the-shelf, task-specific heads for various scene understanding tasks. In this framework, VFM features are treated as a latent space, to which different heads attach to perform specific tasks for future-frame analysis. Extensive experiments show the very strong performance, robustness and scalability of our framework.</p> <hr> <h3 id="learning-to-steer-input-dependent-steering-for-multimodal-llms">Learning to Steer: Input-dependent Steering for Multimodal LLMs</h3> <h5 id="authors-jayneel-parekh-pegah-khayatan-mustafa-shukor-arnaud-dapogny-alasdair-newson-matthieu-cord">Authors: <a href="https://jayneelparekh.github.io/" target="_blank" rel="noopener noreferrer">Jayneel Parekh</a>, <a href="https://pegah-kh.github.io/" target="_blank" rel="noopener noreferrer">Pegah Khayatan</a>, <a href="https://mustafashukor.github.io/" target="_blank" rel="noopener noreferrer">Mustafa Shukor</a>, <a href="https://www.linkedin.com/in/arnaud-dapogny-12653493/" target="_blank" rel="noopener noreferrer">Arnaud Dapogny</a>, <a href="https://sites.google.com/site/alasdairnewson/" target="_blank" rel="noopener noreferrer">Alasdair Newson</a>, <a href="https://cord.isir.upmc.fr/" target="_blank" rel="noopener noreferrer">Matthieu Cord</a> </h5> <h5 align="center"> [<a href="https://arxiv.org/abs/2508.12815" target="_blank" rel="noopener noreferrer">Paper</a>]    [<a href="https://github.com/jayneelparekh/learn-to-steer" target="_blank" rel="noopener noreferrer">Code</a>] </h5> <p><img src="/assets/img/publications/2025_l2s.png" alt="l2s_teaser" height="100%" width="100%"></p> <p>Steering has emerged as a practical approach to enable post-hoc guidance of LLMs towards enforcing a specific behavior. However, it remains largely underexplored for multimodal LLMs (MLLMs); furthermore, existing steering techniques, such as mean steering, rely on a single steering vector, applied independently of the input query. This paradigm faces limitations when the desired behavior is dependent on the example at hand. For example, a safe answer may consist in abstaining from answering when asked for an illegal activity, or may point to external resources or consultation with an expert when asked about medical advice. In this paper, we investigate a fine-grained steering that uses an input-specific linear shift. This shift is computed using contrastive input-specific prompting. However, the input-specific prompts required for this approach are not known at test time. Therefore, we propose to train a small auxiliary module to predict the input-specific steering vector. Our approach, dubbed as L2S (Learn-to-Steer), demonstrates that it reduces hallucinations and enforces safety in MLLMs, outperforming other static baselines.</p> <hr> <h3 id="boosting-generative-image-modeling-via-joint-image-feature-synthesis">Boosting Generative Image Modeling via Joint Image-Feature Synthesis</h3> <h5 id="authors-theodoros-kouzelis-efstathios-karypidis-ioannis-kakogeorgiou-spyros-gidaris-nikos-komodakis">Authors: <a href="https://scholar.google.com/citations?user=a5vkWc8AAAAJ&amp;hl=en" target="_blank" rel="noopener noreferrer">Theodoros Kouzelis</a>, <a href="https://archimedesai.gr/en/researchers/stathis-karypidis" target="_blank" rel="noopener noreferrer">Efstathios Karypidis</a>, <a href="https://scholar.google.com/citations?user=B_dKcz4AAAAJ" target="_blank" rel="noopener noreferrer">Ioannis Kakogeorgiou</a>, <a href="https://gidariss.github.io/" target="_blank" rel="noopener noreferrer">Spyros Gidaris</a>, <a href="https://www.csd.uoc.gr/~komod/" target="_blank" rel="noopener noreferrer">Nikos Komodakis</a> </h5> <h5 align="center"> [<a href="https://arxiv.org/abs/2504.16064" target="_blank" rel="noopener noreferrer">Paper</a>]    [<a href="https://github.com/zelaki/ReDi" target="_blank" rel="noopener noreferrer">Code</a>] </h5> <p><img src="/assets/img/publications/2025_redi/teaser.png" alt="redi_teaser" height="100%" width="100%"></p> <p>Latent diffusion models (LDMs) dominate high-quality image generation, yet integrating representation learning with generative modeling remains a challenge. We introduce a novel generative image modeling framework that seamlessly bridges this gap by leveraging a diffusion model to jointly model low-level image latents (from a variational autoencoder) and high-level semantic features (from a pretrained self-supervised encoder like DINO). Our latent-semantic diffusion approach learns to generate coherent image-feature pairs from pure noise, significantly enhancing both generative quality and training efficiency, all while requiring only minimal modifications to standard Diffusion Transformer architectures. By eliminating the need for complex distillation objectives, our unified design simplifies training and unlocks a powerful new inference strategy: Representation Guidance, which leverages learned semantics to steer and refine image generation. Evaluated in both conditional and unconditional settings, our method delivers substantial improvements in image quality and training convergence speed, establishing a new direction for representation-aware generative modeling.</p> <hr> <h3 id="multi-token-prediction-needs-registers">Multi-Token Prediction Needs Registers</h3> <h5 id="authors-anastasios-gerontopoulos-spyros-gidaris-nikos-komodakis">Authors: <a href="https://scholar.google.com/citations?user=VPTaLcUAAAAJ&amp;hl=en" target="_blank" rel="noopener noreferrer">Anastasios Gerontopoulos</a>, <a href="https://gidariss.github.io/" target="_blank" rel="noopener noreferrer">Spyros Gidaris</a>, <a href="https://www.csd.uoc.gr/~komod/" target="_blank" rel="noopener noreferrer">Nikos Komodakis</a> </h5> <h5 align="center"> [<a href="https://arxiv.org/abs/2505.10518" target="_blank" rel="noopener noreferrer">Paper</a>]    [<a href="https://github.com/nasosger/MuToR" target="_blank" rel="noopener noreferrer">Code</a>] </h5> <p><img src="/assets/img/publications/2025_mutor/teaser.png" alt="mutor_teaser" height="100%" width="100%"></p> <p>Multi-token prediction has emerged as a promising objective for improving language model pretraining, but its benefits have not consistently generalized to other settings such as fine-tuning. In this paper, we propose MuToR, a simple and effective approach to multi-token prediction that interleaves learnable register tokens into the input sequence, each tasked with predicting future targets. Compared to existing methods, MuToR offers several key advantages: it introduces only a negligible number of additional parameters, requires no architectural changes–ensuring compatibility with off-the-shelf pretrained language models–and remains aligned with the next-token pretraining objective, making it especially well-suited for supervised fine-tuning. Moreover, it naturally supports scalable prediction horizons. We demonstrate the effectiveness and versatility of MuToR across a range of use cases, including supervised fine-tuning, parameter-efficient fine-tuning (PEFT), and pretraining, on challenging generative tasks in both language and vision domains.</p> <hr> <h3 id="ipa-an-information-preserving-input-projection-framework-for-efficient-foundation-model-adaptation">IPA: An Information-Preserving Input Projection Framework for Efficient Foundation Model Adaptation</h3> <p class="page-description"><a href="https://sites.google.com/view/ccfm-neurips2025" target="_blank" rel="noopener noreferrer">NeurIPS 2025 Workshop on Continual and Compatible Foundation Model Updates (CCFM)</a></p> <h5 id="authors-yuan-yin-shashanka-venkataramanan-tuan-hung-vu-andrei-bursuc-matthieu-cord">Authors: <a href="https://yuan-yin.github.io/" target="_blank" rel="noopener noreferrer">Yuan Yin</a>, <a href="https://shashankvkt.github.io/" target="_blank" rel="noopener noreferrer">Shashanka Venkataramanan</a>, <a href="https://tuanhungvu.github.io/" target="_blank" rel="noopener noreferrer">Tuan-Hung Vu</a>, <a href="https://abursuc.github.io/" target="_blank" rel="noopener noreferrer">Andrei Bursuc</a>, <a href="https://cord.isir.upmc.fr/" target="_blank" rel="noopener noreferrer">Matthieu Cord</a> </h5> <h5 align="center"> [<a href="https://arxiv.org/abs/2509.04398" target="_blank" rel="noopener noreferrer">Paper</a>]</h5> <p><img src="/assets/img/publications/2025_ipa/ipa.png" alt="ipa_teaser" height="100%" width="100%"></p> <p>Parameter-efficient fine-tuning (PEFT) methods, such as LoRA, reduce adaptation cost by injecting low-rank updates into pretrained weights. However, LoRA’s down-projection is randomly initialized and data-agnostic, discarding potentially useful information. Prior analyses show that this projection changes little during training, while the up-projection carries most of the adaptation, making the random input compression a performance bottleneck. We propose IPA, a feature-aware projection framework that explicitly preserves information in the reduced hidden space. In the linear case, we instantiate IPA with algorithms approximating top principal components, enabling efficient projector pretraining with negligible inference overhead. Across language and vision benchmarks, IPA consistently improves over LoRA and DoRA, achieving on average 1.5 points higher accuracy on commonsense reasoning and 2.3 points on VTAB-1k, while matching full LoRA performance with roughly half the trainable parameters when the projection is frozen.</p> </article> </div> </div> <footer class="sticky-bottom mt-5"> <div class="container">valeo.ai research page </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id="></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","");</script> </body> </html>