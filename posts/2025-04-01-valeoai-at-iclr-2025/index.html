<!DOCTYPE html> <html lang="en"> <head> <meta name="google-site-verification" content=""/> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>valeo.ai at ICLR 2025 | valeo.ai - valeo.ai research page</title> <meta name="author" content=" "/> <meta name="description" content="Amaia Cardiel, Victor Besnier, Yuan Yin, Andrei Bursuc, Éloi Zablocki"/> <meta name="keywords" content="computer vision, ai, valeo, artificial intelligence, research, deep learning"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light"/> <link rel="shortcut icon" href="/assets/img/valeoai_logo_256x256.png"/> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://valeoai.github.io//posts/2025-04-01-valeoai-at-iclr-2025/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous"> <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"> </script> <script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"></script> <script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"[%",right:"%]",display:!0},{left:"$",right:"$",display:!1}]})});</script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><img src="/assets/img/valeoai_logo.png" alt="valeo.ai" class="title-logo" height="24px"></a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/"></a> </li> <li class="nav-item "> <a class="nav-link" href="/team/">Team</a> </li> <li class="nav-item "> <a class="nav-link" href="/research/">Research</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">Code &amp; Data</a> </li> <li class="nav-item "> <a class="nav-link" href="/posts/">Posts</a> </li> <li class="nav-item "> <a class="nav-link" href="/interns/">Internships</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">valeo.ai at ICLR 2025</h1> <p class="post-meta">April 1, 2025 • <span class="read-time" title="Estimated read time"> 6 min read </span></p> <p class="post-tags"> <a href="2025"> <i class="fas fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/category/limited%20supervision"> <i class="fas fa-tag fa-sm"></i> limited supervision</a>   <a href="/category/reliability"> <i class="fas fa-tag fa-sm"></i> reliability</a>   <a href="/category/foundation"> <i class="fas fa-tag fa-sm"></i> foundation</a>   <a href="/category/robustness"> <i class="fas fa-tag fa-sm"></i> robustness</a>   <a href="/category/generalization"> <i class="fas fa-tag fa-sm"></i> generalization</a>   <a href="/category/deep-learning"> <i class="fas fa-tag fa-sm"></i> deep-learning</a>   </p> </header> <article class="post-content"> <p>The <a href="https://iclr.cc/" target="_blank" rel="noopener noreferrer">International Conference on Learning Representations (ICLR)</a> is a leading conference that brings together researchers and practitioners in deep learning, representation learning, and artificial intelligence. It covers a wide range of topics, including optimization, generative models, interpretability, robustness. This year, at the thirteen edition of ICLR, the <a href="../../">valeo.ai</a> team will present 5 papers in the main conference.</p> <p>We will be happy to discuss more about these projects and ideas, and share our exciting ongoing research. Take a quick view of our papers below and come meet us at the posters or catch us for a coffee in the hallways.</p> <hr> <h2 id="halton-scheduler-for-masked-generative-image-transformer">Halton Scheduler For Masked Generative Image Transformer</h2> <h3 id="authors-victor-besnier--mickael-chen--david-hurych--eduardo-valle--matthieu-cord">Authors: <a href="https://scholar.google.com/citations?hl=fr&amp;user=n_C2h-QAAAAJ" target="_blank" rel="noopener noreferrer">Victor Besnier</a>   <a href="https://www.linkedin.com/in/mickael-chen-ml/" target="_blank" rel="noopener noreferrer">Mickael Chen</a>   <a href="https://scholar.google.com/citations?user=XY1PVwYAAAAJ&amp;hl=fr&amp;oi=ao" target="_blank" rel="noopener noreferrer">David Hurych</a>   <a href="https://eduardovalle.com/" target="_blank" rel="noopener noreferrer">Eduardo Valle</a>   <a href="https://cord.isir.upmc.fr/" target="_blank" rel="noopener noreferrer">Matthieu Cord</a> </h3> <h4 align="center"> [<a href="https://arxiv.org/abs/2503.17076" target="_blank" rel="noopener noreferrer">Paper</a>]    [<a href="https://github.com/valeoai/Halton-MaskGIT" target="_blank" rel="noopener noreferrer">Code</a>]    [<a href="https://valeoai.github.io/publications/2025_halton_maskgit">Project page</a>]</h4> <p><img src="/assets/img/publications/2025_halton_maskgit/schematics.png" alt="halton_overview" height="100%" width="100%"></p> <p>Masked Generative Image Transformers (MaskGIT) have gained popularity for their fast and efficient image generation capabilities. However, the sampling strategy used to progressively <em>"unmask"</em> tokens in these models plays a crucial role in determining image quality and diversity. Our new research paper, introduces the <strong>Halton Scheduler</strong>—a novel approach that significantly enhances MaskGIT's image generation performance.</p> <h3>From Confidence to Halton: What’s New?</h3> <p>Traditional MaskGIT uses a Confidence scheduler, which selects tokens based on logit distribution but tends to cluster token selection, leading to reduced image diversity. The Halton Scheduler addresses this by leveraging <strong>low-discrepancy sequences</strong>, the Halton sequence, to distribute token selection more uniformly across the image.</p> <div style="text-align: center;"> <img src="../../assets/img/publications/2025_halton_maskgit/imagenet_quali.png" alt="Halton exemple on ImageNet" style="max-width: 100%; height: auto; border-radius: 5px;"> <p style="font-size: 14px; color: #555;">MaskGIT using our Halton scheduler on ImageNet 256.</p> </div> <h3>Key Insights and Benefits</h3> <ul> <li> <strong>Improved Image Quality and Diversity:</strong> The Halton scheduler reduces clustering of sampled tokens, enhancing image sharpness and background richness.</li> <li> <strong>No Retraining Required:</strong> This scheduler can be integrated as a drop-in replacement for the existing MaskGIT sampling strategy.</li> <li> <strong>Faster and More Balanced Sampling:</strong> By reducing token correlation, the Halton Scheduler allows MaskGIT to progressively add fine details while avoiding local sampling errors.</li> </ul> <div style="text-align: center;"> <img src="../../assets/img/publications/2025_halton_maskgit/txt2img_halton.jpg" alt="Halton exemple" style="max-width: 100%; height: auto; border-radius: 5px;"> <p style="font-size: 14px; color: #555;">Figure 2: MaskGIT using our Halton scheduler for text-to-image.</p> </div> <div style="text-align: center;"> <img src="../../assets/img/publications/2025_halton_maskgit/txt2img_conf.jpg" alt="Confidence exemple" style="max-width: 100%; height: auto; border-radius: 5px;"> <p style="font-size: 14px; color: #555;">Figure 3: MaskGIT using the Confidence scheduler for text-to-image.</p> </div> <h3>Results: ImageNet and COCO Benchmarks</h3> <p>On benchmark datasets like ImageNet (256×256) and COCO, the Halton Scheduler outperforms the baseline Confidence scheduler:</p> <ul> <li> <strong>Reduced Fréchet Inception Distance (FID):</strong> Indicating better image realism.</li> <li> <strong>Improved Precision and Recall:</strong> Reflecting a more diverse image generation.</li> </ul> <hr> <h2 id="llm-wrapper-black-box-semantic-aware-adaptation-of-vision-language-models-for-referring-expression-comprehension">LLM-wrapper: Black-Box Semantic-Aware Adaptation of Vision-Language Models for Referring Expression Comprehension</h2> <h3 id="authors-amaia-cardiel--éloi-zablocki--elias-ramzi--oriane-siméoni--matthieu-cord">Authors: Amaia Cardiel    <a href="https://eloiz.github.io" target="_blank" rel="noopener noreferrer">Éloi Zablocki</a>    <a href="https://elias-ramzi.github.io/" target="_blank" rel="noopener noreferrer">Elias Ramzi</a>    <a href="https://osimeoni.github.io/" target="_blank" rel="noopener noreferrer">Oriane Siméoni</a>    <a href="https://cord.isir.upmc.fr/" target="_blank" rel="noopener noreferrer">Matthieu Cord</a> </h3> <h4 align="center"> [<a href="https://arxiv.org/abs/2409.11919" target="_blank" rel="noopener noreferrer">Paper</a>]    [<a href="https://github.com/valeoai/LLM_wrapper" target="_blank" rel="noopener noreferrer">Code</a>]    [<a href="https://valeoai.github.io/publications/llm_wrapper/">Project page</a>]</h4> <p><img src="/assets/img/publications/2024_llm_wrapper/llm_wrapper_pipeline.png" alt="llm_wrapper_overview" height="100%" width="100%"></p> <p>Vision Language Models (VLMs) have demonstrated remarkable capabilities in various open-vocabulary tasks, yet their zero-shot performance lags behind task-specific fine-tuned models, particularly in complex tasks like Referring Expression Comprehension (REC). Fine-tuning usually requires “white-box” access to the model’s architecture and weights, which is not always feasible due to proprietary or privacy concerns. In this work, we propose LLM-wrapper, a method for “black-box” adaptation of VLMs for the REC task using Large Language Models (LLMs). LLM-wrapper capitalizes on the reasoning abilities of LLMs, improved with a light fine-tuning, to select the most relevant bounding box matching the referring expression, from candidates generated by a zero-shot black-box VLM. Our approach offers several advantages: it enables the adaptation of closed-source models without needing access to their internal workings, it is versatile as it works with any VLM, it transfers to new VLMs and datasets, and it allows for the adaptation of an ensemble of VLMs. We evaluate LLM-wrapper on multiple datasets using different VLMs and LLMs, demonstrating significant performance improvements and highlighting the versatility of our method. While LLM-wrapper is not meant to directly compete with standard white-box fine-tuning, it offers a practical and effective alternative for black-box VLM adaptation.</p> <p><img src="/assets/img/publications/2024_llm_wrapper/llm_wrapper_talk2car.png" alt="llm_wrapper_results" height="100%" width="100%"></p> <hr> <h2 id="moca-self-supervised-representation-learning-by-predicting-masked-online-codebook-assignments">MOCA: Self-supervised Representation Learning by Predicting Masked Online Codebook Assignments</h2> <h3 id="authors-spyros-gidaris-andrei-bursuc--oriane-siméoni--antonin-vobecky--nikos-komodakis--matthieu-cord--patrick-pérez">Authors: <a href="https://scholar.google.com/citations?user=7atfg7EAAAAJ&amp;hl=en" target="_blank" rel="noopener noreferrer">Spyros Gidaris</a>   <a href="https://abursuc.github.io/" target="_blank" rel="noopener noreferrer">Andrei Bursuc</a>   <a href="https://osimeoni.github.io/" target="_blank" rel="noopener noreferrer">Oriane Siméoni</a>    <a href="https://vobecant.github.io/" target="_blank" rel="noopener noreferrer">Antonin Vobecky</a>    <a href="https://www.csd.uoc.gr/~komod/" target="_blank" rel="noopener noreferrer">Nikos Komodakis</a>    <a href="https://cord.isir.upmc.fr/" target="_blank" rel="noopener noreferrer">Matthieu Cord</a>    <a href="https://ptrckprz.github.io/" target="_blank" rel="noopener noreferrer">Patrick Pérez</a> </h3> <h4 align="center"> [<a href="https://arxiv.org/abs/2312.15297" target="_blank" rel="noopener noreferrer">Paper</a>]    [<a href="https://github.com/valeoai/MOCA" target="_blank" rel="noopener noreferrer">Code</a>]    [<a href="https://valeoai.github.io/publications/moca/">Project page</a>]</h4> <p><img src="/assets/img/publications/2024_moca/moca-teaser.png" alt="moca_teaser" height="100%" width="100%"></p> <p>Self-supervised learning can be used for mitigating the greedy needs of Vision Transformer networks for very large fully-annotated datasets. Different classes of self-supervised learning offer representations with either good contextual reasoning properties, e.g., using masked image modeling strategies, or invariance to image perturbations, e.g., with contrastive methods. In this work, we propose a single-stage and standalone method, MOCA, which unifies both desired properties using novel mask-and-predict objectives defined with high-level features (instead of pixel-level details). Moreover, we show how to effectively employ both learning paradigms in a synergistic and computation-efficient way. Doing so, we achieve new state-of-the-art results on low-shot settings and strong experimental results in various evaluation protocols with a training that is at least 3 times faster than prior methods.</p> <hr> <h2 id="learning-a-neural-solver-for-parametric-pdes-to-enhance-physics-informed-methods">Learning a Neural Solver for Parametric PDEs to Enhance Physics-Informed Methods</h2> <h3 id="authors-lise-le-boudec--emmanuel-de-bezenac--louis-serrano--ramon-daniel-regueiro-espino--yuan-yin--patrick-gallinari">Authors: <a href="https://2ailesb.github.io/" target="_blank" rel="noopener noreferrer">Lise Le Boudec</a>    <a href="https://scholar.google.fr/citations?user=KvZw5gYAAAAJ" target="_blank" rel="noopener noreferrer">Emmanuel de Bezenac</a>    <a href="https://scholar.google.com/citations?user=fKlo-lUAAAAJ" target="_blank" rel="noopener noreferrer">Louis Serrano</a>    <a href="https://rd-regueiroespino.github.io/" target="_blank" rel="noopener noreferrer">Ramon Daniel Regueiro-Espino</a>    <a href="https://yuan-yin.github.io" target="_blank" rel="noopener noreferrer">Yuan Yin</a>    <a href="https://pages.isir.upmc.fr/gallinari/" target="_blank" rel="noopener noreferrer">Patrick Gallinari</a> </h3> <h4 align="center"> [<a href="https://arxiv.org/abs/2410.06820" target="_blank" rel="noopener noreferrer">Paper</a>]    [<a href="https://github.com/2ailesB/neural-parametric-solver" target="_blank" rel="noopener noreferrer">Code</a>]    [<a href="https://valeoai.github.io/publications/neural-parametric-solver/">Project page</a>]</h4> <p><img src="/assets/img/publications/2025_neural_parametric_solver.png" alt="neural_solver_pde" height="100%" width="100%"></p> <p>Physics-informed deep learning often faces optimization challenges due to the complexity of solving partial differential equations (PDEs), which involve exploring large solution spaces, require numerous iterations, and can lead to unstable training. These challenges arise particularly from the ill-conditioning of the optimization problem, caused by the differential terms in the loss function. To address these issues, we propose learning a solver, i.e., solving PDEs using a physics-informed iterative algorithm trained on data. Our method learns to condition a gradient descent algorithm that automatically adapts to each PDE instance, significantly accelerating and stabilizing the optimization process and enabling faster convergence of physics-aware models. Furthermore, while traditional physics-informed methods solve for a single PDE instance, our approach addresses parametric PDEs. Specifically, our method integrates the physical loss gradient with the PDE parameters to solve over a distribution of PDE parameters, including coefficients, initial conditions, or boundary conditions. We demonstrate the effectiveness of our method through empirical experiments on multiple datasets, comparing training and test-time optimization performance.</p> <hr> <h2 id="toddlerdiffusion-interactive-structured-image-generation-with-cascaded-schrödinger-bridge">ToddlerDiffusion: Interactive Structured Image Generation with Cascaded Schrödinger Bridge</h2> <h3 id="authors-eslam-abdelrahman--liangbing-zhao--vincent-tao-hu--matthieu-cord--patrick-perez--mohamed-elhoseiny">Authors: Eslam Abdelrahman    Liangbing Zhao    Vincent Tao Hu    <a href="https://cord.isir.upmc.fr/" target="_blank" rel="noopener noreferrer">Matthieu Cord</a>    <a href="https://ptrckprz.github.io/" target="_blank" rel="noopener noreferrer">Patrick Perez</a>    Mohamed Elhoseiny</h3> <h4 align="center"> [<a href="https://arxiv.org/abs/2311.14542" target="_blank" rel="noopener noreferrer">Paper</a>]    [<a href="https://github.com/toddlerdiffusion/code" target="_blank" rel="noopener noreferrer">Code</a>]    [<a href="https://toddlerdiffusion.github.io/website/" target="_blank" rel="noopener noreferrer">Project page</a>]</h4> <p><img src="/assets/img/publications/2025_toddler.PNG" alt="toddlerdiffusion" height="100%" width="100%"></p> <p>Diffusion models break down the challenging task of generating data from high-dimensional distributions into a series of easier denoising steps. Inspired by this paradigm, we propose a novel approach that extends the diffusion framework into modality space, decomposing the complex task of RGB image generation into simpler, interpretable stages. Our method, termed ToddlerDiffusion, cascades modality-specific models, each responsible for generating an intermediate representation, such as contours, palettes, and detailed textures, ultimately culminating in a high-quality RGB image. Instead of relying on the naive LDM concatenation conditioning mechanism to connect the different stages together, we employ Schrödinger Bridge to determine the optimal transport between different modalities. Although employing a cascaded pipeline introduces more stages, which could lead to a more complex architecture, each stage is meticulously formulated for efficiency and accuracy, surpassing Stable-Diffusion (LDM) performance. Modality composition not only enhances overall performance but enables emerging proprieties such as consistent editing, interaction capabilities, high-level interpretability, and faster convergence and sampling rate. Extensive experiments on diverse datasets, including LSUN-Churches, ImageNet, CelebHQ, and LAION-Art, demonstrate the efficacy of our approach, consistently outperforming state-of-the-art methods. For instance, ToddlerDiffusion achieves notable efficiency, matching LDM performance on LSUN-Churches while operating 2× faster with a 3× smaller architecture.</p> </article> </div> </div> <footer class="sticky-bottom mt-5"> <div class="container">valeo.ai research page </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id="></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","");</script> </body> </html>