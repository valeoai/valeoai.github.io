<!DOCTYPE html> <html lang="en"> <head> <meta name="google-site-verification" content=""/> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>valeo.ai at ECCV 2024 | valeo.ai - valeo.ai research page</title> <meta name="author" content=" "/> <meta name="description" content="Björn Michele, Amaia Cardiel, Yuan Yin, Yihong Xu, Nermin Samet, Tuan-Hung Vu, Andrei Bursuc, Éloi Zablocki"/> <meta name="keywords" content="computer vision, ai, valeo, artificial intelligence, research, deep learning"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light"/> <link rel="shortcut icon" href="/assets/img/valeoai_logo_256x256.png"/> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://valeoai.github.io//posts/2024-09-25-valeoai-at-eccv-2024/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous"> <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"> </script> <script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"></script> <script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"[%",right:"%]",display:!0},{left:"$",right:"$",display:!1}]})});</script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><img src="/assets/img/valeoai_logo.png" alt="valeo.ai" class="title-logo" height="24px"></a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/"></a> </li> <li class="nav-item "> <a class="nav-link" href="/team/">Team</a> </li> <li class="nav-item "> <a class="nav-link" href="/research/">Research</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">Code &amp; Data</a> </li> <li class="nav-item "> <a class="nav-link" href="/posts/">Posts</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">valeo.ai at ECCV 2024</h1> <p class="post-meta">September 25, 2024 • <span class="read-time" title="Estimated read time"> 13 min read </span></p> <p class="post-tags"> <a href="2024"> <i class="fas fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/category/3d%20perception"> <i class="fas fa-tag fa-sm"></i> 3d perception</a>   <a href="/category/multi-sensor"> <i class="fas fa-tag fa-sm"></i> multi-sensor</a>   <a href="/category/limited%20supervision"> <i class="fas fa-tag fa-sm"></i> limited supervision</a>   <a href="/category/reliability"> <i class="fas fa-tag fa-sm"></i> reliability</a>   <a href="/category/motion%20forecasting"> <i class="fas fa-tag fa-sm"></i> motion forecasting</a>   <a href="/category/robustness"> <i class="fas fa-tag fa-sm"></i> robustness</a>   <a href="/category/generalization"> <i class="fas fa-tag fa-sm"></i> generalization</a>   <a href="/category/driving"> <i class="fas fa-tag fa-sm"></i> driving</a>   </p> </header> <article class="post-content"> <p>The <a href="https://eccv.ecva.net/" target="_blank" rel="noopener noreferrer">European Conference on Computer Vision (ECCV)</a> is a biennial landmark conference for the increasingly large community of researchers in computer vision and machine learning from both academia and industry. At the 2024 edition the valeo.ai team will present five papers in the main conference and four in the workshops. We are also organizing two tutorials (<a href="https://uqtutorial.github.io/" target="_blank" rel="noopener noreferrer">Bayesian Odyssey</a> and <a href="https://shashankvkt.github.io/eccv2024-SSLBIG-tutorial.github.io/" target="_blank" rel="noopener noreferrer">Time is precious: Self-Supervised Learning Beyond Images</a>), the <a href="https://uncertainty-cv.github.io/2024/" target="_blank" rel="noopener noreferrer">Uncertainty Quantification for Computer Vision</a> workshop, a talk at the <a href="https://sites.google.com/view/omnilabel-workshop-eccv24/program" target="_blank" rel="noopener noreferrer">OmniLabel workshop</a>, and the <a href="https://github.com/valeoai/bravo_challenge" target="_blank" rel="noopener noreferrer">BRAVO challenge</a>. Our team has also contributed to the reviewing process, with seven reviewers, three area chairs, and two outstanding reviewer awards. The team will be at ECCV to present these works and will be happy to discuss more about these projects and ideas, and share our exciting ongoing research.</p> <p><img src="/assets/img/posts/2024_eccv/valeoai_eccv.jfif" alt="valeo.ai team at ECCV 2024" height="100%" width="100%"></p> <h2 id="train-till-you-drop-towards-stable-and-robust-source-free-unsupervised-3d-domain-adaptation">Train Till You Drop: Towards Stable and Robust Source-free Unsupervised 3D Domain Adaptation</h2> <h3 id="authors-bjoern-michele--alexandre-boulch--tuan-hung-vu--gilles-puy--renaud-marlet--nicolas-courty">Authors: <a href="https://bjoernmichele.com" target="_blank" rel="noopener noreferrer">Bjoern Michele</a>    <a href="https://boulch.eu/" target="_blank" rel="noopener noreferrer">Alexandre Boulch</a>    <a href="https://tuanhungvu.github.io/" target="_blank" rel="noopener noreferrer">Tuan-Hung Vu</a>    <a href="https://sites.google.com/site/puygilles/" target="_blank" rel="noopener noreferrer">Gilles Puy</a>    <a href="http://imagine.enpc.fr/~marletr/" target="_blank" rel="noopener noreferrer">Renaud Marlet</a>    <a href="https://people.irisa.fr/Nicolas.Courty/" target="_blank" rel="noopener noreferrer">Nicolas Courty</a> </h3> <h4 align="center"> [<a href="https://arxiv.org/abs/2409.04409" target="_blank" rel="noopener noreferrer">Paper</a>]    [<a href="https://github.com/valeoai/TTYD" target="_blank" rel="noopener noreferrer">Code</a>]    [<a href="https://valeoai.github.io/publications/ttyd/">Project page</a>]</h4> <p><img src="/assets/img/publications/2024_ttyd/featured.png" alt="ttdy_overview" height="100%" width="100%"></p> <div class="caption"> <b>Evolution of the performance of baselines without degradation prevention strategies as they train over 20k iterations.</b> Our method (TTYDcore) uses an unsupervised criterion to stop training. The horizontal dotted line illustrates that we keep the model obtained at the stopping point (marked with a cross). </div> <p>We tackle the challenging problem of source-free unsupervised domain adaptation (SFUDA) for 3D semantic segmentation. It amounts to performing domain adaptation on an unlabeled target domain without any access to source data; the available information is a model trained to achieve good performance on the source domain. A common issue with existing SFUDA approaches is that performance degrades after some training time, which is a by-product of an under-constrained and ill-posed problem. We discuss two strategies to alleviate this issue. First, we propose a sensible way to regularize the learning problem. Second, we introduce a novel criterion based on agreement with a reference model. It is used (1) to stop the training when appropriate and (2) as validator to select hyperparameters without any knowledge on the target domain. Our contributions are easy to implement and readily amenable for all SFUDA methods, ensuring stable improvements over all baselines. We validate our findings on various 3D lidar settings, achieving state-of-the-art performance.</p> <p><img src="/assets/img/posts/2024_eccv/ttyd_results.PNG" alt="ttyd_results" height="100%" width="100%"></p> <div class="caption"> <b>Examples of results with TTYDstop</b>: ground truth (GT), initial model trained only on source data, training with our training scheme when using our stopping criterion, and “full” training for 20k iterations. Notable errors due to degradation are marked with a dashed rectangle. </div> <hr> <h2 id="unitraj-a-unified-framework-for-scalable-vehicle-trajectory-prediction">UniTraj: A Unified Framework for Scalable Vehicle Trajectory Prediction</h2> <h4 id="authors-lan-feng--mohammadhossein-bahari--kaouther-messaoud-ben-amor--éloi-zablocki--matthieu-cord--alexandre-alahi">Authors: <a href="https://alan-lanfeng.github.io/" target="_blank" rel="noopener noreferrer">Lan Feng</a>    <a href="https://mohammadhossein-bahari.github.io/" target="_blank" rel="noopener noreferrer">Mohammadhossein Bahari</a>    <a href="https://scholar.google.com/citations?user=X0teZIAAAAAJ" target="_blank" rel="noopener noreferrer">Kaouther Messaoud Ben Amor</a>    <a href="https://scholar.google.fr/citations?user=dOkbUmEAAAAJ" target="_blank" rel="noopener noreferrer">Éloi Zablocki</a>    <a href="https://cord.isir.upmc.fr/" target="_blank" rel="noopener noreferrer">Matthieu Cord</a>    <a href="https://people.epfl.ch/alexandre.alahi" target="_blank" rel="noopener noreferrer">Alexandre Alahi</a> </h4> <h4 align="center"> [<a href="https://arxiv.org/abs/2403.15098" target="_blank" rel="noopener noreferrer">Paper</a>]    [<a href="https://github.com/vita-epfl/unitraj" target="_blank" rel="noopener noreferrer">Code</a>]    [<a href="https://vita-epfl.github.io/UniTraj/" target="_blank" rel="noopener noreferrer">Project page</a>]    [<a href="https://www.youtube.com/watch?v=2IzuUtiNA_4" target="_blank" rel="noopener noreferrer">Video</a>]</h4> <p>Vehicle trajectory prediction has increasingly relied on data-driven solutions, but their ability to scale to different data domains and the impact of larger dataset sizes on their generalization remain under-explored. While these questions can be studied by employing multiple datasets, it is challenging due to several discrepancies, e.g., in data formats, map resolution, and semantic annotation types. To address these challenges, we introduce UniTraj, a comprehensive framework that unifies various datasets, models, and evaluation criteria, presenting new opportunities for the vehicle trajectory prediction field. In particular, using UniTraj, we conduct extensive experiments and find that model performance significantly drops when transferred to other datasets. However, enlarging data size and diversity can substantially improve performance, leading to a new state-of-the-art result for the nuScenes dataset. We provide insights into dataset characteristics to explain these findings.</p> <p><img src="/assets/img/publications/2024_unitraj/unitraj.PNG" alt="unitraj_overview" height="100%" width="100%"></p> <div class="caption"> <b>Overview of UniTraj: a unified platform for comprehensive research in trajectory prediction.</b> </div> <hr> <h2 id="lost-and-found-overcoming-detector-failures-in-online-multi-object-tracking">Lost and Found: Overcoming Detector Failures in Online Multi-Object Tracking</h2> <h4 id="authors-lorenzo-vaquero--yihong-xu--xavier-alameda-pineda--víctor-m-brea--manuel-mucientes">Authors: <a href="https://citius.gal/team/lorenzo-vaquero-otal/" target="_blank" rel="noopener noreferrer">Lorenzo Vaquero</a>    <a href="https://github.com/yihongXU" target="_blank" rel="noopener noreferrer">Yihong Xu</a>    <a href="https://xavirema.eu/" target="_blank" rel="noopener noreferrer">Xavier Alameda-Pineda</a>    <a href="https://citius.gal/team/victor-manuel-brea-sanchez/" target="_blank" rel="noopener noreferrer">Víctor M. Brea</a>    <a href="https://persoal.citius.usc.es/manuel.mucientes/" target="_blank" rel="noopener noreferrer">Manuel Mucientes</a> </h4> <h4 align="center"> [<a href="https://arxiv.org/abs/2407.10151" target="_blank" rel="noopener noreferrer">Paper</a>]    [<a href="https://github.com/lorenzovaquero/BUSCA" target="_blank" rel="noopener noreferrer">Code</a>]    [<a href="https://valeoai.github.io/publications/busca/">Project page</a>]</h4> <p>Multi-object tracking (MOT) endeavors to precisely estimate the positions and identities of multiple objects over time. The prevailing approach, tracking-by-detection (TbD), first detects objects and then links detections, resulting in a simple yet effective method. However, contemporary detectors may occasionally miss some objects in certain frames, causing trackers to cease tracking prematurely. To tackle this issue, we propose BUSCA, meaning ‘to search’, a versatile framework compatible with any online TbD system, enhancing its ability to persistently track those objects missed by the detector, primarily due to occlusions. Remarkably, this is accomplished without modifying past tracking results or accessing future frames, i.e., in a fully online manner. BUSCA generates proposals based on neighboring tracks, motion, and learned tokens. Utilizing a decision Transformer that integrates multimodal visual and spatiotemporal information, it addresses the object-proposal association as a multi-choice question-answering task. BUSCA is trained independently of the underlying tracker, solely on synthetic data, without requiring fine-tuning. Through BUSCA, we showcase consistent performance enhancements across five different trackers and establish a new state-of-the-art baseline across three different benchmarks.</p> <p><img src="/assets/img/publications/2024_busca/busca.png" alt="busca_overview" height="90%" width="90%"></p> <div class="caption"> <b>Overview of BUSCA</b>: Enhancing multi-object trackers by finding undetected objects. </div> <hr> <h2 id="clip-dinoiser-teaching-clip-a-few-dino-tricks-for-open-vocabulary-semantic-segmentation">CLIP-DINOiser: Teaching CLIP a few DINO tricks for open-vocabulary semantic segmentation</h2> <h4 id="authors-monika-wysoczańska-oriane-siméoni--michaël-ramamonjisoa-andrei-bursuc-tomasz-trzciński--patrick-pérez">Authors: <a href="https://wysoczanska.github.io/" target="_blank" rel="noopener noreferrer">Monika Wysoczańska</a>  <a href="https://osimeoni.github.io/" target="_blank" rel="noopener noreferrer">Oriane Siméoni</a>   <a href="https://michaelramamonjisoa.github.io/" target="_blank" rel="noopener noreferrer">Michaël Ramamonjisoa</a>  <a href="https://abursuc.github.io/" target="_blank" rel="noopener noreferrer">Andrei Bursuc</a>  <a href="https://scholar.google.com/citations?hl=en&amp;user=bJMRBFoAAAAJ" target="_blank" rel="noopener noreferrer">Tomasz Trzciński</a>   <a href="https://ptrckprz.github.io/" target="_blank" rel="noopener noreferrer">Patrick Pérez</a> </h4> <h4 align="center"> [<a href="https://arxiv.org/abs/2312.12359" target="_blank" rel="noopener noreferrer">Paper</a>]    [<a href="https://github.com/wysoczanska/clip_dinoiser/" target="_blank" rel="noopener noreferrer">Code</a>]    [<a href="https://wysoczanska.github.io/CLIP_DINOiser/" target="_blank" rel="noopener noreferrer">Project page</a>]</h4> <p>The popular CLIP model displays impressive zero-shot capabilities thanks to its seamless interaction with arbitrary text prompts. However, its lack of spatial awareness makes it unsuitable for dense computer vision tasks, e.g., semantic segmentation, without an additional fine-tuning step that often uses annotations and can potentially suppress its original open-vocabulary properties. Meanwhile, self-supervised representation methods have demonstrated good localization properties without human-made annotations nor explicit supervision. In this work, we take the best of both worlds and propose an open-vocabulary semantic segmentation method, which does not require any annotations.</p> <p><img src="/assets/img/publications/2024_dinoiser/dinoiser-examples.png" alt="dinoiser_example" height="100%" width="100%"></p> <div class="caption"> <b>Examples of open-vocabulary semantic segmentation results obtained with our method CLIP-DINOiser on ‘in-the-wild’ images vs. those of MaskCLIP.</b> </div> <p>We propose to locally improve dense MaskCLIP features, which are computed with a simple modification of CLIP’s last pooling layer, by integrating localization priors extracted from self-supervised features from DINO. By doing so, we greatly improve the performance of MaskCLIP and produce smooth outputs. Moreover, we show that the used self-supervised feature properties can directly be learnt from CLIP features. Our method CLIP-DINOiser needs only a single forward pass of CLIP and two light convolutional layers at inference, no extra supervision nor extra memory and reaches state-of-the-art results on challenging and fine-grained benchmarks such as COCO, Pascal Context, Cityscapes and ADE20k.</p> <p><img src="/assets/img/posts/2024_eccv/dinoiser_overview.PNG" alt="dinoiser_overview" height="100%" width="100%"></p> <div class="caption"> <b>Overview of CLIP-DINOiser.</b> We use DINO as a teacher which ‘teaches’ CLIP how to extract localization information with similar patch correlations. At inference, an input image is forwarded through the frozen CLIP image backbone and MaskCLIP projection. The produced features are then improved with our pooling strategy which is guided by correlations predicted with a trained convolutional layer applied on CLIP.</div> <hr> <h2 id="reliability-in-semantic-segmentation-can-we-use-synthetic-data">Reliability in Semantic Segmentation: Can We Use Synthetic Data</h2> <h4 id="authors-thibaut-loiseau---tuan-hung-vu---mickael-chen--patrick-pérez--matthieu-cord">Authors: <a href="https://imagine-lab.enpc.fr/staff-members/thibaut-loiseau/" target="_blank" rel="noopener noreferrer">Thibaut Loiseau</a>    <a href="https://tuanhungvu.github.io/" target="_blank" rel="noopener noreferrer">Tuan-Hung Vu</a>    <a href="https://scholar.google.fr/citations?user=QnRpMJAAAAAJ" target="_blank" rel="noopener noreferrer">Mickael Chen</a>    <a href="https://ptrckprz.github.io/" target="_blank" rel="noopener noreferrer">Patrick Pérez</a>    <a href="https://cord.isir.upmc.fr/" target="_blank" rel="noopener noreferrer">Matthieu Cord</a> </h4> <h4 align="center"> [<a href="https://arxiv.org/abs/2312.09231" target="_blank" rel="noopener noreferrer">Paper</a>]    [<a href="https://github.com/valeoai/GenVal" target="_blank" rel="noopener noreferrer">Code</a>]    [<a href="https://valeoai.github.io/publications/genval">Project page</a>]</h4> <p>Assessing the reliability of perception models to covariate shifts and out-of-distribution (OOD) detection is crucial for safety-critical applications such as autonomous vehicles. By nature of the task, however, the relevant data is difficult to collect and annotate. In this paper, we challenge cutting-edge generative models to automatically synthesize data for assessing reliability in semantic segmentation. By fine-tuning Stable Diffusion, we perform zero-shot generation of synthetic data in OOD domains or inpainted with OOD objects. Synthetic data is employed to provide an initial assessment of pretrained segmenters, thereby offering insights into their performance when confronted with real edge cases. Through extensive experiments, we demonstrate a high correlation between the performance on synthetic data and the performance on real OOD data, showing the validity approach. Furthermore, we illustrate how synthetic data can be utilized to enhance the calibration and OOD detection capabilities of segmenters.</p> <p><img src="/assets/img/publications/2024_genval/genval-overview.PNG" alt="genval_overview" height="100%" width="100%"></p> <div class="caption"> <b>Assessing 40 pretrained segmenters under covariate shifts.</b> Segmentation models under scrutiny were trained on Cityscapes train set only (in-domain data). They are evaluated on (i) Cityscapes validation set, (ii) real OOD data, and (iii) proposed synthetic data. We observe a strong correlation between results on (ii) and (iii). </div> <hr> <h2 id="valeo4cast-a-modular-approach-to-end-to-end-forecasting">Valeo4Cast: A Modular Approach to End-to-End Forecasting</h2> <p class="page-description"><a href="https://sites.google.com/view/road-eccv2024/home" target="_blank" rel="noopener noreferrer">ECCV 2024 ROAD++ Workshop</a></p> <p class="page-description"><a href="https://www.argoverse.org/E2EForecasting.html" target="_blank" rel="noopener noreferrer">Winning solution in Argoverse 2 Unified Detection, Tracking and Forecasting Challenge</a></p> <h4 id="authors-yihong-xu-éloi-zablocki--alexandre-boulch-gilles-puy---mickaël-chen-florent-bartoccioni--nermin-samet---oriane-siméoni---spyros-gidaris---tuan-hung-vu-andrei-bursuc--eduardo-valle-renaud-marlet--matthieu-cord">Authors: <a href="https://scholar.google.fr/citations?user=vMLRRVkAAAAJ" target="_blank" rel="noopener noreferrer">Yihong Xu</a>, <a href="https://scholar.google.fr/citations?user=dOkbUmEAAAAJ" target="_blank" rel="noopener noreferrer">Éloi Zablocki</a>, <a href="https://www.boulch.eu/" target="_blank" rel="noopener noreferrer">Alexandre Boulch</a>, <a href="https://sites.google.com/site/puygilles/home" target="_blank" rel="noopener noreferrer">Gilles Puy</a>, <a href="https://scholar.google.com/citations?user=QnRpMJAAAAAJ" target="_blank" rel="noopener noreferrer">Mickaël Chen</a>, <a href="https://f-barto.github.io/" target="_blank" rel="noopener noreferrer">Florent Bartoccioni</a>, <a href="https://nerminsamet.github.io/" target="_blank" rel="noopener noreferrer">Nermin Samet</a>, <a href="https://osimeoni.github.io/" target="_blank" rel="noopener noreferrer">Oriane Siméoni</a>, <a href="https://scholar.google.fr/citations?user=7atfg7EAAAAJ" target="_blank" rel="noopener noreferrer">Spyros Gidaris</a>, <a href="https://tuanhungvu.github.io/" target="_blank" rel="noopener noreferrer">Tuan-Hung Vu</a>, <a href="https://abursuc.github.io/" target="_blank" rel="noopener noreferrer">Andrei Bursuc</a>, <a href="https://scholar.google.com/citations?user=lxWPqWAAAAAJ" target="_blank" rel="noopener noreferrer">Eduardo Valle</a>, <a href="http://imagine.enpc.fr/~marletr/" target="_blank" rel="noopener noreferrer">Renaud Marlet</a>, <a href="https://cord.isir.upmc.fr/" target="_blank" rel="noopener noreferrer">Matthieu Cord</a> </h4> <h4 align="center"> [<a href="https://arxiv.org/abs/2406.08113" target="_blank" rel="noopener noreferrer">Paper</a>]    [<a href="https://eval.ai/web/challenges/challenge-page/2006/leaderboard/4752" target="_blank" rel="noopener noreferrer">leaderboard</a>]    [<a href="https://valeoai.github.io/publications/valeo4cast/">page</a>]</h4> <p>Motion forecasting is crucial in autonomous driving systems to anticipate the future trajectories of surrounding agents such as pedestrians, vehicles, and traffic signals. In end-to-end forecasting, the model must jointly detect from sensor data (cameras or LiDARs) the position and past trajectories of the different elements of the scene and predict their future location. We depart from the current trend of tackling this task via end-to-end training from perception to forecasting and we use a modular approach instead. Following a recent study, we individually build and train detection, tracking, and forecasting modules. We then only use consecutive finetuning steps to integrate the modules better and alleviate compounding errors. Our study reveals that this simple yet effective approach significantly improves performance on the end-to-end forecasting benchmark. Consequently, our solution ranks first in the Argoverse 2 end-to-end Forecasting Challenge held at CVPR 2024 Workshop on Autonomous Driving (WAD), with 63.82 mAPf. We surpass forecasting results by +17.1 points over last year’s winner and by +13.3 points over this year’s runner-up. This remarkable performance in forecasting can be explained by our modular paradigm, which integrates finetuning strategies and significantly outperforms the end-to-end-trained counterparts.</p> <p><img src="/assets/img/publications/2024_valeo4cast/valeo4cast.PNG" alt="valeo4cast_overview" height="80%" width="80%"></p> <div class="caption"> <b>Valeo4Cast overview.</b> </div> <hr> <h2 id="pafuse-part-based-diffusion-for-3d-whole-body-pose-estimation">PAFUSE: Part-based Diffusion for 3D Whole-Body Pose Estimation</h2> <p class="page-description"><a href="https://sites.google.com/view/t-cap-2024/home" target="_blank" rel="noopener noreferrer">ECCV 2024 Workshop Towards a Complete Analysis of People (T-CAP)</a></p> <h4 id="authors-nermin-samet--cédric-rommel--david-picard--eduardo-valle">Authors: <a href="https://nerminsamet.github.io/" target="_blank" rel="noopener noreferrer">Nermin Samet</a>    <a href="https://cedricrommel.github.io/" target="_blank" rel="noopener noreferrer">Cédric Rommel</a>    <a href="https://davidpicard.github.io/" target="_blank" rel="noopener noreferrer">David Picard</a>    <a href="https://eduardovalle.com/" target="_blank" rel="noopener noreferrer">Eduardo Valle</a> </h4> <h4 align="center"> [<a href="https://arxiv.org/abs/2407.10220" target="_blank" rel="noopener noreferrer">Paper</a>]    [<a href="https://github.com/valeoai/PAFUSE" target="_blank" rel="noopener noreferrer">Code</a>]    [<a href="https://valeoai.github.io/publications/pafuse">Project page</a>]</h4> <p>We introduce a novel approach for 3D whole-body pose estimation, addressing the challenge of scale- and deformability- variance across body parts brought by the challenge of extending the 17 major joints on the human body to fine-grained keypoints on the face and hands. In addition to addressing the challenge of exploiting motion in unevenly sampled data, we combine stable diffusion to a hierarchical part representation which predicts the relative locations of fine-grained keypoints within each part (e.g., face) with respect to the part’s local reference frame. On the H3WB dataset, our method greatly outperforms the current state of the art, which fails to exploit the temporal information. We also show considerable improvements compared to other spatiotemporal 3D human-pose estimation approaches that fail to account for the body part specificities.</p> <p><img src="/assets/img/posts/2024_eccv/pafuse.PNG" alt="pafuse_overview" height="100%" width="100%"></p> <div class="caption"> <b>Overview of PAFUSE.</b> </div> <hr> <h2 id="llm-wrapper-black-box-semantic-aware-adaptation-of-vision-language-foundation-models">LLM-wrapper: Black-Box Semantic-Aware Adaptation of Vision-Language Foundation Models</h2> <p class="page-description"><a href="https://sites.google.com/view/eval-fomo-24/home" target="_blank" rel="noopener noreferrer">ECCV 2024 Workshop on Emergent Visual Abilities and Limits of Foundation Models (Eval-FoMo)</a></p> <h4 id="authors-amaia-cardiel--éloi-zablocki--oriane-siméoni--elias-ramzi--matthieu-cord">Authors: Amaia Cardiel    <a href="https://scholar.google.fr/citations?user=dOkbUmEAAAAJ" target="_blank" rel="noopener noreferrer">Éloi Zablocki</a>    <a href="https://osimeoni.github.io/" target="_blank" rel="noopener noreferrer">Oriane Siméoni</a>    <a href="https://elias-ramzi.github.io/" target="_blank" rel="noopener noreferrer">Elias Ramzi</a>    <a href="https://cord.isir.upmc.fr/" target="_blank" rel="noopener noreferrer">Matthieu Cord</a> </h4> <h4 align="center"> [<a href="https://arxiv.org/abs/2409.11919" target="_blank" rel="noopener noreferrer">Paper</a>]    [<a href="https://valeoai.github.io/publications/llm_wrapper">Project page</a>]</h4> <p>Vision Language Models (VLMs) have shown impressive performances on numerous tasks but their zero-shot capabilities can be limited compared to dedicated or fine-tuned models. Yet, fine-tuning VLMs comes with strong limitations as it requires a ‘white-box’ access to the model’s architecture and weights while some recent models are proprietary (e.g., Grounding DINO 1.5). It also requires expertise to design the fine-tuning objectives and optimize the hyper-parameters, which are specific to each VLM and downstream task. In this work, we propose LLM-wrapper, a novel approach to adapt VLMs in a ‘black-box’ and semantic-aware manner by leveraging large language models (LLMs) so as to reason on their outputs.</p> <p><img src="/assets/img/publications/2024_llm_wrapper/llm_wrapper.PNG" alt="llm-wrapper_overview" height="80%" width="80%"></p> <div class="caption"> <b>Overview of LLM-Wrapper.</b> </div> <p>We demonstrate the effectiveness of LLM-wrapper on Referring Expression Comprehension (REC), a challenging open-vocabulary task that requires spatial and semantic reasoning. Our approach significantly boosts the performance of off-the-shelf models, yielding results that are on par or competitive when compared with classic VLM fine-tuning (cf ‘FT VLM’ in our main results). Despite a few failure cases due to the LLM ‘blindness’ (cf Qualitative results, bottom right)), LLM-wrapper shows better semantic, spatial and relational reasoning, as illustrated in our qualitative results below.</p> <p><img src="/assets/img/posts/2024_eccv/llm_wrapper_results.PNG" alt="llm-wrapper_results" height="90%" width="90%"></p> <div class="caption"> <b>LLM-Wrapper results.</b> </div> <hr> <h2 id="regents-real-world-safety-critical-driving-scenario-generation-made-stable">ReGentS: Real-World Safety-Critical Driving Scenario Generation Made Stable</h2> <p class="page-description"><a href="https://coda-dataset.github.io/w-coda2024/" target="_blank" rel="noopener noreferrer">ECCV 2024 Workshop on Multimodal Perception and Comprehension of Corner Cases in Autonomous Driving (W-CODA)</a></p> <h4 id="authors-yuan-yin--pegah-khayatan--éloi-zablocki--alexandre-boulch--matthieu-cord">Authors: <a href="https://yuan-yin.github.io/" target="_blank" rel="noopener noreferrer">Yuan Yin</a>    <a href="https://pegah-kh.github.io/" target="_blank" rel="noopener noreferrer">Pegah Khayatan</a>    <a href="https://scholar.google.fr/citations?user=dOkbUmEAAAAJ" target="_blank" rel="noopener noreferrer">Éloi Zablocki</a>    <a href="https://www.boulch.eu/" target="_blank" rel="noopener noreferrer">Alexandre Boulch</a>    <a href="https://cord.isir.upmc.fr/" target="_blank" rel="noopener noreferrer">Matthieu Cord</a> </h4> <h4 align="center"> [<a href="https://arxiv.org/abs/2409.07830" target="_blank" rel="noopener noreferrer">Paper</a>]    [<a href="https://valeoai.github.io/publications/regents">Project page</a>]</h4> <p>Machine learning based autonomous driving systems often face challenges with safety-critical scenarios that are rare in real-world data, hindering their large-scale deployment. While increasing real-world training data coverage could address this issue, it is costly and dangerous. This work explores generating safety-critical driving scenarios by modifying complex real-world regular scenarios through trajectory optimization. We propose ReGentS, which stabilizes generated trajectories and introduces heuristics to avoid obvious collisions and optimization problems. Our approach addresses unrealistic diverging trajectories and unavoidable collision scenarios that are not useful for training robust planner. We also extend the scenario generation framework to handle real-world data with up to 32 agents. Additionally, by using a differentiable simulator, our approach simplifies gradient descent-based optimization involving a simulator, paving the way for future advancements.</p> <p><img src="/assets/img/publications/2024_regents/regents_page.png" alt="regents_overview" height="100%" width="100%"></p> <hr> <h2 id="the-bravo-semantic-segmentation-challenge-results-in-uncv2024">The BRAVO Semantic Segmentation Challenge Results in UNCV2024</h2> <p class="page-description"><a href="https://uncertainty-cv.github.io/2024/challenge/" target="_blank" rel="noopener noreferrer">ECCV 2024 Workshop on Uncertainty Quantification for Computer Vision</a></p> <h4 id="authors-tuan-hung-vu---eduardo-valle--andrei-bursuc--tommie-kerssies--daan-de-geus--gijs-dubbelman--long-qian--bingke-zhu--yingying-chen--ming-tang--jinqiao-wang--tomáš-vojíř--jan-šochman--jiří-matas--michael-smith--frank-ferrie--shamik-basu--christos-sakaridis--luc-van-gool">Authors: <a href="https://tuanhungvu.github.io/" target="_blank" rel="noopener noreferrer">Tuan-Hung Vu</a>    <a href="https://eduardovalle.com/" target="_blank" rel="noopener noreferrer">Eduardo Valle</a>    <a href="https://abursuc.github.io/" target="_blank" rel="noopener noreferrer">Andrei Bursuc</a>    <a href="">Tommie Kerssies</a>    <a href="">Daan de Geus</a>    <a href="">Gijs Dubbelman</a>    <a href="">Long Qian</a>    <a href="">Bingke Zhu</a>    <a href="">Yingying Chen</a>    <a href="">Ming Tang</a>    <a href="">Jinqiao Wang</a>    <a href="">Tomáš Vojíř</a>    <a href="">Jan Šochman</a>    <a href="">Jiří Matas</a>    <a href="">Michael Smith</a>    <a href="">Frank Ferrie</a>    <a href="">Shamik Basu</a>    <a href="">Christos Sakaridis</a>    <a href="">Luc Van Gool</a> </h4> <h4 align="center"> [<a href="https://arxiv.org/abs/2409.15107" target="_blank" rel="noopener noreferrer">Paper</a>]    [<a href="https://github.com/valeoai/bravo_challenge" target="_blank" rel="noopener noreferrer">Code</a>]    [<a href="https://benchmarks.elsa-ai.eu/?ch=1&amp;com=introduction" target="_blank" rel="noopener noreferrer">Project page</a>]</h4> <p>We propose the unified BRAVO challenge to benchmark the reliability of semantic segmentation models under realistic perturbations and unknown out-of-distribution (OOD) scenarios. We define two categories of reliability: (1) semantic reliability, which reflects the model’s accuracy and calibration when exposed to various perturbations; and (2) OOD reliability, which measures the model’s ability to detect object classes that are unknown during training. The challenge attracted nearly 100 submissions from international teams representing notable research institutions. The results reveal interesting insights into the importance of large-scale pre-training and minimal architectural design in developing robust and reliable semantic segmentation models.</p> <p><img src="/assets/img/publications/2024_bravo/bravo-overview.PNG" alt="bravo_overview" height="80%" width="80%"></p> <div class="caption"> <b>All submissions.</b> Aggregated metrics (out-of-distribution and semantic) on axes, ranking metric (BRAVO Index) on level set. More freedom on the training dataset (Task 2, in orange) did not translate into better results. </div> </article> </div> </div> <footer class="sticky-bottom mt-5"> <div class="container">valeo.ai research page </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id="></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","");</script> </body> </html>