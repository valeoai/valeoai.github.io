<!DOCTYPE html> <html lang="en"> <head> <meta name="google-site-verification" content=""/> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>PLOP: Probabilistic poLynomial Objects trajectory Prediction for autonomous driving | valeo.ai - valeo.ai research page</title> <meta name="author" content=" "/> <meta name="description" content="Thibault Buhet, Emilie Wirbel, Andrei Bursuc and Xavier Perrotton"/> <meta name="keywords" content="computer vision, ai, valeo, artificial intelligence, research, deep learning"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light"/> <link rel="shortcut icon" href="/assets/img/valeoai_logo_256x256.png"/> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://valeoai.github.io//posts/plop"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous"> <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"> </script> <script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"></script> <script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"[%",right:"%]",display:!0},{left:"$",right:"$",display:!1}]})});</script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><img src="/assets/img/valeoai_logo.png" alt="valeo.ai" class="title-logo" height="24px"></a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/"></a> </li> <li class="nav-item "> <a class="nav-link" href="/team/">Team</a> </li> <li class="nav-item "> <a class="nav-link" href="/research/">Research</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">Code &amp; Data</a> </li> <li class="nav-item "> <a class="nav-link" href="/posts/">Posts</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">PLOP: Probabilistic poLynomial Objects trajectory Prediction for autonomous driving</h1> <p class="post-meta">November 26, 2020 • <span class="read-time" title="Estimated read time"> 11 min read </span></p> <p class="post-tags"> <a href="2020"> <i class="fas fa-calendar fa-sm"></i> 2020 </a>   ·   <a href="/category/driving"> <i class="fas fa-tag fa-sm"></i> driving</a>   <a href="/category/multi-sensor"> <i class="fas fa-tag fa-sm"></i> multi-sensor</a>   </p> </header> <article class="post-content"> <p><em>This post describes our <a href="https://drive.google.com/file/d/1--QAL2sR7KMk9R4DwxyfJAT5iGCheFrn/view" target="_blank" rel="noopener noreferrer">recent work</a> on probabilistic trajectory prediction for autonomous driving presented at <a href="https://www.robot-learning.org/home" target="_blank" rel="noopener noreferrer">CORL 2020</a>. PLOP is a trajectory prediction method that intent to control an autonomous vehicle (ego vehicle) in urban environment while considering and predicting the intents of other road users (neighbors). We focus here on predicting multiple feasible future trajectories for both ego vehicle and neighbors through a probabilistic framework and rely on a conditional imitation learning algorithm, conditioned by a navigation command for the ego vehicle (e.g., ``turn right’’). Our model processes only onboard sensor data (camera and lidars) along with detections of past and presents objects relaxing the necessity of an HDMap and is computationally efficient as it can run in real time (25 fps) on an embedded board in the real vehicle. We evaluate our method offline on the publicly available dataset nuScenes <a class="citation" href="#holger2020nuscenes">(Caesar et al., 2020)</a>, achieving state-of-the-art performance, investigate the impact of our architecture choices on online simulated experiments and show preliminary insights for real vehicle control.</em></p> <p><img src="/assets/img/posts/plop/plop.png" alt="plop_teaser" height="60%" width="60%"></p> <div class="caption"> <b>Figure 1. Qualitative example of trajectory predictions on a test sample from nuScenes dataset.</b> The top image show a bird's eye view of PLOP's predictions for the ego and neighbor vehicles (to be compared with the ground truth in green). The bottom row present the input image (left) in which we added object correspondance with the bird's eye view and the auxiliary semantic segmentation of this image (right)</div> <p>Predicting the future positions of other agents of the road, or of the autonomous vehicle itself, is critical for autonomous driving. This trajectory prediction must not only respect the rules of the road, but capture the interactions of the agents over time. It is also important to allow multiple possible predictions, as there is usually not a single valid trajectory.</p> <p>Some approaches such as ChauffeurNet <a class="citation" href="#bansal2018chauffeutnet">(Bansal et al., 2018)</a> use a high-levelscene representation (road map, traffic lights, speed limit, route, dynamic bounding boxes, etc.). More recently, MultiPath <a class="citation" href="#chai2019multipath">(Chai et al., 2019)</a> uses trajectory anchors, used in one-step object detection, extracted from the training data for ego vehicle prediction. <a class="citation" href="#hong2019rules">(Hong et al., 2019)</a> use a high level representation which includes some dynamic context. In contrast, we choose to leverage also low level sensor data, here Lidar point clouds and camera image. In that domain, recent approaches address the variation in agent behaviors by predicting multiple trajectories, often in a stochastic way. Many works, e.g., PRECOG <a class="citation" href="#rhinehart2019precog">(Rhinehart et al., 2019)</a>, MFP <a class="citation" href="#tang2019mfp">(Tang &amp; Salakhutdinov, 2019)</a>, SocialGAN <a class="citation" href="#gupta2018socialgan">(Gupta et al., 2018)</a> and others <a class="citation" href="#rhinehart2018deepim">(Rhinehart et al., 2018)</a>, focus on this aspect through a probabilistic framework on the network output or latent representations, producing multiple trajectories for ego vehicle, nearby vehicles or both. <a class="citation" href="#phan2020covernet">(Phan-Minh et al., 2020)</a> generate a trajectory set, then classify correct trajectories. <a class="citation" href="#marchetti2020mantra">(Marchetti et al., 2020)</a> generate multiple futures from encodings of similar trajectories stored in a memory. <a class="citation" href="#ohn2020learning">(Ohn-Bar et al., 2020)</a> learn a weighted mixture of expert policies trained to mimic agents with specific behaviors. In PRECOG, <a class="citation" href="#rhinehart2019precog">(Rhinehart et al., 2019)</a> advance a probabilistic formulation that explicitly models interactions between agents, using latent variables to model their plausible reactions, with the possibility to precondition the trajectory of the ego vehicle by a goal.</p> <h2 id="plop-method">PLOP method</h2> <h3 id="contributions">Contributions</h3> <p>Our main goal is to produce a trajectory prediction which can be used to drive the ego vehicle relying on a conditional imitation learning algorithm, conditioned by a navigation command for the ego vehicle (e.g., “turn right”). To do so, we propose a single-shot, anchor-less trajectory prediction method, based on Mixture Desity Networks (MDNs) and polynomial trajectory constraints, relying only on on-board sensors which relaxes the HD map requirement and allow more flexibility for driving in the real world. The polynomial formulation ensures that the predicted trajectories are coherent and smooth, while providing more learning flexibility through the extra parameters. We find that this mitigates training instability and mode collapse that are common to MDNs <a class="citation" href="#cui2019multimodal">(Cui et al., 2019)</a>. PLOP is trainable end-to-end from imitation learning, where data is relatively easier to obtain and it is computationally efficient during both training and inference as it predicts trajectory coefficients in a single step, without requiring a RNN-based decoder. The polynomial function trajectory coefficients eschew the need for anchors <a class="citation" href="#chai2019multipath">(Chai et al., 2019)</a>, whose quality can vary across datasets.</p> <p>We propose an extensive evaluation of PLOP and show its effectiveness across datasets and settings. We conduct a comparison showing the improvement over state-of-the-art PRECOG <a class="citation" href="#rhinehart2019precog">(Rhinehart et al., 2019)</a> on the public dataset nuScenes <a class="citation" href="#holger2020nuscenes">(Caesar et al., 2020)</a>;</p> <p>Then for a better evaluation of the driving capacities of PLOP, we study closed loop performance for the ego vehicle, on simulation and with preliminary insights for real vehicle control.</p> <h3 id="network-architecture">Network architecture</h3> <p>PLOP takes as inputs: the ego and neighbor vehicles past positions represented as time sequences of x and y over the last 2 seconds, the frontal camera image of the ego vehicle, and 2 second history of bird’s eye views with a cell resolution of 1m square containing the lidar point cloud and the object detections information represented in Figure 2. The objects detections being the output of a state of the art perception algorithm.</p> <p><img src="/assets/img/posts/plop/inputs_plop_post.PNG" alt="plop_inputs" height="100%" width="100%"></p> <div class="caption"> <b>Figure 2. Image and Bird's eye view.</b> The left image is an example of a front camera input image of PLOP and the diagram on the right is a representation of the bird'eye view input.</div> <p>We pass these inputs through a multibranch neural network represented in Figure 3 to predict the ego vehicle future trajectory and two auxiliary tasks that are the future trajectory prediction for the neighbors vehicles and the semantic segmentation of the camera image.</p> <p><img src="/assets/img/posts/plop/archi_outputs.png" alt="plop_archi_outputs" height="100%" width="100%"></p> <div class="caption"> <b>Figure 3. PLOP's Architecture.</b> PLOP's architecture is reprented on the left while the polynomial multimodal gaussian trajectory representation is on the right</div> <p>The front camera image features, the bird’s eye view features and the ego vehicle past positions features are passed down to conditional fully connected architecture to output multiple future trajectories for the ego vehicle regarding the current navigation order. The trajectories are predicted using MDNs where gaussian means are generated using polynomial functions of degree 4 over x and y</p> <p>To improve the learning stability of our training and inject awareness about the scene layouts into the camera features we pass them through a U-Net decoder to output semantic segmentation and then use an auxiliary cross entropy loss. To improve the encoding of interactions between the differents agents of the scene in the bird’s eye features, we predict the future possible trajectories for each neighbor feeding the bird’s eye views encoding and its past positions encoded through a LSTM layer to a small fully connected network. The weights of LSTMs and fully connected layers are shared between all neighbors. This output allows us to get useful information about the ego vehicle environment that can be used online to improve the ego vehicle driving with safety collision checks for example.</p> <h2 id="offline-evaluation">Offline evaluation</h2> <p>To evaluate PLOP, we use the nuScenes dataset to train the trajectory loss along with the Audi <a class="citation" href="#geyer2019a2d2">(Geyer et al., 2019)</a> dataset to train the semantic segmentation loss. We choose to compare our method with the DESIRE <a class="citation" href="#lee2017desire">(Lee et al., 2017)</a> baseline and against two state of the art methods that are PRECOG and ESP <a class="citation" href="#rhinehart2019precog">(Rhinehart et al., 2019)</a> using the minimum Mean Squared Deviation metric to avoid penalizing valid trajectories that are not matching the ground truth. For one agent, meaning ego vehicle only, PRECOG and ESP have access to the future desired target position and PRECOG return significantly better results than PLOP but PLOP still reaches similar results as ESP. For multiple agents PLOP outperforms other presented methods . We note that the comparison if fairer for neighbor trajectories and the performance is relevant since they are by definition open loop.</p> <p><img src="/assets/img/posts/plop/offline_results.PNG" alt="plop_offline_results" height="60%" width="60%"></p> <div class="caption"> <b>Figure 4. Comparison with state-of-the-art:</b> Against the DESIRE, ESP and PRECOG for predicting a trajectory of 4 seconds into the future</div> <p>But we argue that such evaluation is not totally relevant for controling the ego vehicle in real conditions. Such metrics does not value the situations in which the errors are made, failing to brake at a traffic light is a critical error for example but it is quick and represent a very small part of the test set so it will impact very poorly the overall metrics. However, making a small constant error such as driving 2kph too slow over the whole test set set might be an acceptable and non impacting error but will lead to a considerable overall error. Also, using only offline metrics where the method can’t control the vehicle does not allow us to evaluate its capacities to react to its own mistakes.</p> <h2 id="online-evaluation-through-simulation">Online Evaluation through simulation</h2> <p>To simulate driving, we developped a data driven simulator that allows us to use real driving data to simulate applying the prediction to the ego vehicle. We can generate the input data that corresponds to the new vehicle position after following the trajectory using reprojections (for the image and the pointcloud), then use it to predict a new trajectory, and so on. This allows us the measure the performance in closed loop, and in particular to count failures which would have resulted in a takeover. We rely on 3 metrics: lateral (&gt;1m from expert), high speed (catching up to a vehicle 15% faster than the real vehicle up to 0.6s in the future) and low speed (&gt; 20kph under the expert speed) errors count.</p> <p><img src="/assets/img/posts/plop/simu_plop.png" alt="plop_online_results" height="100%" width="100%"></p> <div class="caption"> <b>Figure 5. Evaluation using the simulator.</b> Comparison with PLOP without semantic segmentation loss, Constant velocity baseline and Multi-Layer Perceptron baseline in the table on the left. Additionnal qualitative results about the errors positioning on the differents test tracks are on the right.</div> <p>We trained PLOP on an internal dataset combining both open road and urban test track and compared PLOP, PLOP without auxiliary semantic loss, the constant velocity baseline and a MLP baseline in our simulator using test data. We note that semantic segmentation improve the driving performance and that MLP has better offline metrics than constant velocity approach but still perform worse due to the simulated driving conditions. As expected, offline metrics are not discriminating enough for the online behavior since the best model checkpoints in simulation are not necessarily the ones with the better offline metrics. An additionnal ablation study where we remove mandatory information (such as the camera image input) shows that it may even be dangerous to trust them blindly.</p> <div class="publication-teaser"> <iframe width="560" height="315" src="https://www.youtube.com/embed/94FwahFmc5A?start=94" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe> </div> <h2 id="conclusion">Conclusion</h2> <p>In this work, we demonstrate the interest of our multi-input multimodal approach PLOP for vehicle trajectory prediction in an urban environment. Our architecture leverages frontal camera and Lidar inputs, to produce multiple trajectories using reparameterized Mixture Density Networks, with an auxiliary semantic segmentation task. We show that we can improve open loop state-of-the-art performance in a multi-agent system, by evaluating the vehicle trajectories from the nuScenes dataset. We also provide a simulated closed loop evaluation, to go towards real vehicle online application. Please check out our paper along with supplementary materials for greater details about our approach and experiments and feel free to contact us for any question.</p> <h2 id="references">References</h2> <ol class="bibliography"> <li> <div class="row"> Holger Caesar, Varun Bankiti, Alex H. Lang, Sourabh Vora, Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Giancarlo Baldan, and Oscar Beijbom.  nuScenes: A Multimodal Dataset for Autonomous Driving.  <em>In cvpr</em>, 2020. </div> </li> <li> <div class="row"> Mayank Bansal, Alex Krizhevsky, and Abhijit S. Ogale.  ChauffeurNet: Learning to Drive by Imitating the Best and Synthesizing the Worst.  <em>CoRR</em>, 2018. </div> </li> <li> <div class="row"> Yuning Chai, Benjamin Sapp, Mayank Bansal, and Dragomir Anguelov.  MultiPath: Multiple Probabilistic Anchor Trajectory Hypotheses for Behavior Prediction.  Oct 2019. </div> </li> <li> <div class="row"> Joey Hong, Benjamin Sapp, and James Philbin.  Rules of the Road: Predicting Driving Behavior with a Convolutional Model of Semantic Interactions.  <em>CoRR</em>, 2019. </div> </li> <li> <div class="row"> Nicholas Rhinehart, Rowan McAllister, Kris Kitani, and Sergey Levine.  Precog: Prediction conditioned on goals in visual multi-agent settings.  <em>In iccv</em>, 2019. </div> </li> <li> <div class="row"> Charlie Tang, and Russ R Salakhutdinov.  Multiple futures prediction.  <em>In Advances in Neural Information Processing Systems</em>, 2019. </div> </li> <li> <div class="row"> Agrim Gupta, Justin Johnson, Li Fei-Fei, Silvio Savarese, and Alexandre Alahi.  Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks.  <em>CoRR</em>, 2018. </div> </li> <li> <div class="row"> Nicholas Rhinehart, Rowan McAllister, and Sergey Levine.  Deep Imitative Models for Flexible Inference, Planning, and Control.  <em>CoRR</em>, 2018. </div> </li> <li> <div class="row"> Tung Phan-Minh, Elena Corina Grigore, Freddy A Boulton, Oscar Beijbom, and Eric M Wolff.  Covernet: Multimodal behavior prediction using trajectory sets.  <em>In cvpr</em>, 2020. </div> </li> <li> <div class="row"> Francesco Marchetti, Federico Becattini, Lorenzo Seidenari, and Alberto Del Bimbo.  Mantra: Memory augmented networks for multiple trajectory prediction.  <em>In cvpr</em>, 2020. </div> </li> <li> <div class="row"> Eshed Ohn-Bar, Aditya Prakash, Aseem Behl, Kashyap Chitta, and Andreas Geiger.  Learning Situational Driving.  <em>In cvpr</em>, 2020. </div> </li> <li> <div class="row"> Henggang Cui, Vladan Radosavljevic, Fang-Chieh Chou, Tsung-Han Lin, Thi Nguyen, Tzu-Kuo Huang, Jeff Schneider, and Nemanja Djuric.  Multimodal trajectory predictions for autonomous driving using deep convolutional networks.  <em>In icra</em>, 2019. </div> </li> <li> <div class="row"> Jakob Geyer, Yohannes Kassahun, Mentar Mahmudi, Xavier Ricou, Rupesh Durgesh, Andrew S. Chung, Lorenz Hauswald, Viet Hoang Pham, Maximilian Mühlegg, Sebastian Dorn, Tiffany Fernandez, Martin Jänicke, Sudesh Mirashi, Chiragkumar Savani, Martin Sturm, Oleksandr Vorobiov, and Peter Schuberth.  A2D2: AEV Autonomous Driving Dataset.  2019. </div> </li> <li> <div class="row"> Namhoon Lee, Wongun Choi, Paul Vernaza, Chris Choy, Philip Torr, and Manmohan Chandraker.  DESIRE: Distant Future Prediction in Dynamic Scenes with Interacting Agents.  <em>In </em>, Jul 2017. </div> </li> </ol> </article> </div> </div> <footer class="sticky-bottom mt-5"> <div class="container">valeo.ai research page </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id="></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","");</script> </body> </html>